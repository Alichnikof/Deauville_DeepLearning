{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to train using all the best parameters for each split (but using different models) 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters per split\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop over each row (each split) and build the command using the tuned parameters\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row['split'])\n",
    "    # Build checkpoint path as before\n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    \n",
    "    # Get hyperparameters from the CSV row.\n",
    "    lr = row['params_lr']              # e.g., 0.0000143417\n",
    "    batch_size = row['params_batch_size']  # e.g., 128\n",
    "    wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "    optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "    # Use the ft_mode field to decide which flag to pass:\n",
    "    ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "    ft_flag = \"\"\n",
    "    if ft_mode == \"finetune\":\n",
    "        ft_flag = \"--finetune\"\n",
    "    elif ft_mode == \"transfer_learning\":\n",
    "        ft_flag = \"--transfer_learning\"\n",
    "    # for full_retrain, no additional flag is needed\n",
    "    \n",
    "    # If lr_scheduler is True, add that flag.\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # If balance flag is True, add that flag.\n",
    "    balance_flag = \"\"\n",
    "    if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "        balance_flag = \"--balance\"\n",
    "    \n",
    "    # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Construct the command string\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 4 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "        f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "        f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar but just the same checkpoint, probably best approach. 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Set the CUDA device if desired\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# # Path to the CSV file with best parameters per split\n",
    "# csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "# best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# # Loop over each row (each split) and build the command using the tuned parameters\n",
    "# for _, row in best_params.iterrows():\n",
    "#     split = int(row['split'])\n",
    "#     # Build checkpoint path as before\n",
    "#     checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth\"\n",
    "    \n",
    "#     # Get hyperparameters from the CSV row.\n",
    "#     lr = row['params_lr']              # e.g., 0.0000143417\n",
    "#     batch_size = row['params_batch_size']  # e.g., 128\n",
    "#     wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "#     optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "#     # Use the ft_mode field to decide which flag to pass:\n",
    "#     ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "#     ft_flag = \"\"\n",
    "#     if ft_mode == \"finetune\":\n",
    "#         ft_flag = \"--finetune\"\n",
    "#     elif ft_mode == \"transfer_learning\":\n",
    "#         ft_flag = \"--transfer_learning\"\n",
    "#     # for full_retrain, no additional flag is needed\n",
    "    \n",
    "#     # If lr_scheduler is True, add that flag.\n",
    "#     lr_scheduler_flag = \"\"\n",
    "#     if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "#         lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "#     # If balance flag is True, add that flag.\n",
    "#     balance_flag = \"\"\n",
    "#     if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "#         balance_flag = \"--balance\"\n",
    "    \n",
    "#     # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "#     augm_flag = \"--augm 4\"\n",
    "    \n",
    "#     # Construct the command string\n",
    "#     command = (\n",
    "#         f\"python train.py --split_index {split} --run 5 --nepochs 20 \"\n",
    "#         f\"--checkpoint {checkpoint_path} \"\n",
    "#         f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "#         f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "#         f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "#     )\n",
    "#     print(\"Running command:\", command)\n",
    "#     os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different checkpoint modle but same parameter run 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=0, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2078, val:548\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1648  430]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8406\tVal AUC: 0.8110\n",
      "Epoch: [1/20]\tLoss: 0.549390\tTrain AUC: 0.8755\tVal AUC: 0.8244\n",
      "Epoch: [2/20]\tLoss: 0.455384\tTrain AUC: 0.8953\tVal AUC: 0.8235\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.391793\tTrain AUC: 0.9140\tVal AUC: 0.8273\n",
      "Epoch: [4/20]\tLoss: 0.389372\tTrain AUC: 0.9282\tVal AUC: 0.8334\n",
      "Epoch: [5/20]\tLoss: 0.365978\tTrain AUC: 0.9421\tVal AUC: 0.8364\n",
      "Epoch: [6/20]\tLoss: 0.305674\tTrain AUC: 0.9530\tVal AUC: 0.8329\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.287366\tTrain AUC: 0.9613\tVal AUC: 0.8280\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.273340\tTrain AUC: 0.9720\tVal AUC: 0.8283\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.237707\tTrain AUC: 0.9795\tVal AUC: 0.8229\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=1, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2114, val:512\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1648  466]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8456\tVal AUC: 0.8047\n",
      "Epoch: [1/20]\tLoss: 0.552388\tTrain AUC: 0.8797\tVal AUC: 0.8283\n",
      "Epoch: [2/20]\tLoss: 0.424402\tTrain AUC: 0.8990\tVal AUC: 0.8208\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.381357\tTrain AUC: 0.9199\tVal AUC: 0.8065\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.346973\tTrain AUC: 0.9332\tVal AUC: 0.8006\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.325576\tTrain AUC: 0.9478\tVal AUC: 0.7943\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.295639\tTrain AUC: 0.9610\tVal AUC: 0.7874\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=2, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2118, val:508\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1672  446]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8197\tVal AUC: 0.8276\n",
      "Epoch: [1/20]\tLoss: 0.575882\tTrain AUC: 0.8606\tVal AUC: 0.8453\n",
      "Epoch: [2/20]\tLoss: 0.481747\tTrain AUC: 0.8852\tVal AUC: 0.8268\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.431447\tTrain AUC: 0.9091\tVal AUC: 0.8288\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.364836\tTrain AUC: 0.9217\tVal AUC: 0.8343\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.357089\tTrain AUC: 0.9379\tVal AUC: 0.8200\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.327750\tTrain AUC: 0.9504\tVal AUC: 0.8271\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=3, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2164, val:462\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1716  448]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8325\tVal AUC: 0.8150\n",
      "Epoch: [1/20]\tLoss: 0.513122\tTrain AUC: 0.8695\tVal AUC: 0.8540\n",
      "Epoch: [2/20]\tLoss: 0.447882\tTrain AUC: 0.8904\tVal AUC: 0.8643\n",
      "Epoch: [3/20]\tLoss: 0.388506\tTrain AUC: 0.9065\tVal AUC: 0.8701\n",
      "Epoch: [4/20]\tLoss: 0.384871\tTrain AUC: 0.9224\tVal AUC: 0.8741\n",
      "Epoch: [5/20]\tLoss: 0.351571\tTrain AUC: 0.9363\tVal AUC: 0.8796\n",
      "Epoch: [6/20]\tLoss: 0.328190\tTrain AUC: 0.9503\tVal AUC: 0.8817\n",
      "Epoch: [7/20]\tLoss: 0.292057\tTrain AUC: 0.9601\tVal AUC: 0.8785\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [8/20]\tLoss: 0.290125\tTrain AUC: 0.9745\tVal AUC: 0.8800\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [9/20]\tLoss: 0.225765\tTrain AUC: 0.9821\tVal AUC: 0.8808\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [10/20]\tLoss: 0.214008\tTrain AUC: 0.9873\tVal AUC: 0.8862\n",
      "Epoch: [11/20]\tLoss: 0.192973\tTrain AUC: 0.9927\tVal AUC: 0.8866\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.146533\tTrain AUC: 0.9956\tVal AUC: 0.8921\n",
      "Epoch: [13/20]\tLoss: 0.126699\tTrain AUC: 0.9967\tVal AUC: 0.8861\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=4, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2106, val:520\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1664  442]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8360\tVal AUC: 0.8424\n",
      "Epoch: [1/20]\tLoss: 0.570872\tTrain AUC: 0.8727\tVal AUC: 0.8530\n",
      "Epoch: [2/20]\tLoss: 0.449398\tTrain AUC: 0.8939\tVal AUC: 0.8458\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.404939\tTrain AUC: 0.9105\tVal AUC: 0.8424\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.371764\tTrain AUC: 0.9222\tVal AUC: 0.8331\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.365165\tTrain AUC: 0.9324\tVal AUC: 0.8264\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.336843\tTrain AUC: 0.9465\tVal AUC: 0.8240\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=5, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1664  446]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8334\tVal AUC: 0.8390\n",
      "Epoch: [1/20]\tLoss: 0.539701\tTrain AUC: 0.8735\tVal AUC: 0.8425\n",
      "Epoch: [2/20]\tLoss: 0.468715\tTrain AUC: 0.8972\tVal AUC: 0.8329\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.417670\tTrain AUC: 0.9164\tVal AUC: 0.8335\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.372925\tTrain AUC: 0.9309\tVal AUC: 0.8344\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.353136\tTrain AUC: 0.9447\tVal AUC: 0.8227\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.310291\tTrain AUC: 0.9550\tVal AUC: 0.8348\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=6, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1636  474]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8428\tVal AUC: 0.8393\n",
      "Epoch: [1/20]\tLoss: 0.530197\tTrain AUC: 0.8750\tVal AUC: 0.8641\n",
      "Epoch: [2/20]\tLoss: 0.466429\tTrain AUC: 0.8959\tVal AUC: 0.8627\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.377910\tTrain AUC: 0.9125\tVal AUC: 0.8633\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.386818\tTrain AUC: 0.9289\tVal AUC: 0.8611\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.348712\tTrain AUC: 0.9417\tVal AUC: 0.8599\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.310790\tTrain AUC: 0.9524\tVal AUC: 0.8615\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=7, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1672  460]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8158\tVal AUC: 0.8683\n",
      "Epoch: [1/20]\tLoss: 0.646995\tTrain AUC: 0.8570\tVal AUC: 0.9095\n",
      "Epoch: [2/20]\tLoss: 0.473786\tTrain AUC: 0.8827\tVal AUC: 0.9122\n",
      "Epoch: [3/20]\tLoss: 0.448104\tTrain AUC: 0.9020\tVal AUC: 0.9105\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.391525\tTrain AUC: 0.9154\tVal AUC: 0.9138\n",
      "Epoch: [5/20]\tLoss: 0.366358\tTrain AUC: 0.9292\tVal AUC: 0.9091\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.350749\tTrain AUC: 0.9420\tVal AUC: 0.9115\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.316186\tTrain AUC: 0.9553\tVal AUC: 0.9067\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.295944\tTrain AUC: 0.9653\tVal AUC: 0.8959\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=8, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1666  466]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8396\tVal AUC: 0.7947\n",
      "Epoch: [1/20]\tLoss: 0.516908\tTrain AUC: 0.8809\tVal AUC: 0.7971\n",
      "Epoch: [2/20]\tLoss: 0.459035\tTrain AUC: 0.9045\tVal AUC: 0.7961\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.403779\tTrain AUC: 0.9216\tVal AUC: 0.8058\n",
      "Epoch: [4/20]\tLoss: 0.355319\tTrain AUC: 0.9320\tVal AUC: 0.8221\n",
      "Epoch: [5/20]\tLoss: 0.344029\tTrain AUC: 0.9472\tVal AUC: 0.8160\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.319197\tTrain AUC: 0.9588\tVal AUC: 0.8272\n",
      "Epoch: [7/20]\tLoss: 0.295581\tTrain AUC: 0.9692\tVal AUC: 0.8322\n",
      "Epoch: [8/20]\tLoss: 0.260998\tTrain AUC: 0.9765\tVal AUC: 0.8310\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.239798\tTrain AUC: 0.9837\tVal AUC: 0.8347\n",
      "Epoch: [10/20]\tLoss: 0.199956\tTrain AUC: 0.9880\tVal AUC: 0.8383\n",
      "Epoch: [11/20]\tLoss: 0.173828\tTrain AUC: 0.9915\tVal AUC: 0.8467\n",
      "Epoch: [12/20]\tLoss: 0.154935\tTrain AUC: 0.9950\tVal AUC: 0.8507\n",
      "Epoch: [13/20]\tLoss: 0.140480\tTrain AUC: 0.9967\tVal AUC: 0.8485\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [14/20]\tLoss: 0.111677\tTrain AUC: 0.9976\tVal AUC: 0.8548\n",
      "Epoch: [15/20]\tLoss: 0.099567\tTrain AUC: 0.9984\tVal AUC: 0.8494\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=9, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2058, val:568\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1626  432]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8200\tVal AUC: 0.8090\n",
      "Epoch: [1/20]\tLoss: 0.550712\tTrain AUC: 0.8669\tVal AUC: 0.8400\n",
      "Epoch: [2/20]\tLoss: 0.472437\tTrain AUC: 0.8922\tVal AUC: 0.8372\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.408748\tTrain AUC: 0.9104\tVal AUC: 0.8419\n",
      "Epoch: [4/20]\tLoss: 0.386444\tTrain AUC: 0.9244\tVal AUC: 0.8417\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.360440\tTrain AUC: 0.9374\tVal AUC: 0.8391\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.333340\tTrain AUC: 0.9511\tVal AUC: 0.8335\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.294055\tTrain AUC: 0.9598\tVal AUC: 0.8303\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# FINE TUNED MODEL\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "for split in range(0, 10): \n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 12 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --finetune --augm 4 --oversample\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch - run 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=0, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2078, val:548\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1648  430]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5040\tVal AUC: 0.5221\n",
      "Epoch: [1/20]\tLoss: 0.620240\tTrain AUC: 0.8200\tVal AUC: 0.7609\n",
      "Epoch: [2/20]\tLoss: 0.507803\tTrain AUC: 0.8141\tVal AUC: 0.7090\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.485530\tTrain AUC: 0.8386\tVal AUC: 0.7530\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.386629\tTrain AUC: 0.9215\tVal AUC: 0.7627\n",
      "Epoch: [5/20]\tLoss: 0.349227\tTrain AUC: 0.9349\tVal AUC: 0.7607\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.284103\tTrain AUC: 0.9541\tVal AUC: 0.7294\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.246772\tTrain AUC: 0.9711\tVal AUC: 0.7452\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=1, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2114, val:512\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1648  466]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5653\tVal AUC: 0.4889\n",
      "Epoch: [1/20]\tLoss: 0.614890\tTrain AUC: 0.7179\tVal AUC: 0.5893\n",
      "Epoch: [2/20]\tLoss: 0.544252\tTrain AUC: 0.8184\tVal AUC: 0.6313\n",
      "Epoch: [3/20]\tLoss: 0.498902\tTrain AUC: 0.8662\tVal AUC: 0.6537\n",
      "Epoch: [4/20]\tLoss: 0.463514\tTrain AUC: 0.8383\tVal AUC: 0.6569\n",
      "Epoch: [5/20]\tLoss: 0.476533\tTrain AUC: 0.8657\tVal AUC: 0.6156\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.436542\tTrain AUC: 0.5461\tVal AUC: 0.5298\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.511618\tTrain AUC: 0.8913\tVal AUC: 0.6062\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.399309\tTrain AUC: 0.9211\tVal AUC: 0.5872\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.348278\tTrain AUC: 0.9412\tVal AUC: 0.5996\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=2, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2118, val:508\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1672  446]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4601\tVal AUC: 0.4874\n",
      "Epoch: [1/20]\tLoss: 0.582925\tTrain AUC: 0.6328\tVal AUC: 0.6156\n",
      "Epoch: [2/20]\tLoss: 0.529003\tTrain AUC: 0.7895\tVal AUC: 0.7293\n",
      "Epoch: [3/20]\tLoss: 0.533441\tTrain AUC: 0.8181\tVal AUC: 0.7606\n",
      "Epoch: [4/20]\tLoss: 0.485973\tTrain AUC: 0.8354\tVal AUC: 0.7637\n",
      "Epoch: [5/20]\tLoss: 0.439044\tTrain AUC: 0.8379\tVal AUC: 0.7599\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.467379\tTrain AUC: 0.8017\tVal AUC: 0.7463\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.393867\tTrain AUC: 0.9359\tVal AUC: 0.7845\n",
      "Epoch: [8/20]\tLoss: 0.319028\tTrain AUC: 0.9532\tVal AUC: 0.8070\n",
      "Epoch: [9/20]\tLoss: 0.264318\tTrain AUC: 0.9634\tVal AUC: 0.8119\n",
      "Epoch: [10/20]\tLoss: 0.243355\tTrain AUC: 0.9697\tVal AUC: 0.7934\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.226425\tTrain AUC: 0.9766\tVal AUC: 0.7883\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.200006\tTrain AUC: 0.9838\tVal AUC: 0.7958\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=3, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2164, val:462\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1716  448]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4336\tVal AUC: 0.4533\n",
      "Epoch: [1/20]\tLoss: 0.592816\tTrain AUC: 0.7975\tVal AUC: 0.7586\n",
      "Epoch: [2/20]\tLoss: 0.517479\tTrain AUC: 0.8333\tVal AUC: 0.7598\n",
      "Epoch: [3/20]\tLoss: 0.467638\tTrain AUC: 0.8474\tVal AUC: 0.6436\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.396238\tTrain AUC: 0.8652\tVal AUC: 0.6739\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.340662\tTrain AUC: 0.9578\tVal AUC: 0.7794\n",
      "Epoch: [6/20]\tLoss: 0.273676\tTrain AUC: 0.9726\tVal AUC: 0.7892\n",
      "Epoch: [7/20]\tLoss: 0.226113\tTrain AUC: 0.9828\tVal AUC: 0.8082\n",
      "Epoch: [8/20]\tLoss: 0.186448\tTrain AUC: 0.9871\tVal AUC: 0.7880\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.142110\tTrain AUC: 0.9922\tVal AUC: 0.7854\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.116666\tTrain AUC: 0.9938\tVal AUC: 0.7765\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=4, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2106, val:520\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1664  442]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4672\tVal AUC: 0.4449\n",
      "Epoch: [1/20]\tLoss: 0.672140\tTrain AUC: 0.6638\tVal AUC: 0.6143\n",
      "Epoch: [2/20]\tLoss: 0.541836\tTrain AUC: 0.8294\tVal AUC: 0.7417\n",
      "Epoch: [3/20]\tLoss: 0.492173\tTrain AUC: 0.7808\tVal AUC: 0.6979\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.470831\tTrain AUC: 0.8298\tVal AUC: 0.7161\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.397808\tTrain AUC: 0.9245\tVal AUC: 0.7684\n",
      "Epoch: [6/20]\tLoss: 0.316031\tTrain AUC: 0.9413\tVal AUC: 0.7722\n",
      "Epoch: [7/20]\tLoss: 0.291192\tTrain AUC: 0.9594\tVal AUC: 0.7718\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.231999\tTrain AUC: 0.9713\tVal AUC: 0.7596\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.222477\tTrain AUC: 0.9751\tVal AUC: 0.7612\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=5, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1664  446]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5206\tVal AUC: 0.5560\n",
      "Epoch: [1/20]\tLoss: 0.592995\tTrain AUC: 0.7225\tVal AUC: 0.6459\n",
      "Epoch: [2/20]\tLoss: 0.490534\tTrain AUC: 0.8494\tVal AUC: 0.7413\n",
      "Epoch: [3/20]\tLoss: 0.434786\tTrain AUC: 0.8656\tVal AUC: 0.7089\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.383067\tTrain AUC: 0.8665\tVal AUC: 0.6680\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.285173\tTrain AUC: 0.9715\tVal AUC: 0.7810\n",
      "Epoch: [6/20]\tLoss: 0.218167\tTrain AUC: 0.9834\tVal AUC: 0.7759\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.177772\tTrain AUC: 0.9897\tVal AUC: 0.7681\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.151131\tTrain AUC: 0.9927\tVal AUC: 0.7753\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=6, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1636  474]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5371\tVal AUC: 0.5528\n",
      "Epoch: [1/20]\tLoss: 0.632031\tTrain AUC: 0.7672\tVal AUC: 0.7853\n",
      "Epoch: [2/20]\tLoss: 0.536225\tTrain AUC: 0.7686\tVal AUC: 0.7484\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.512542\tTrain AUC: 0.7714\tVal AUC: 0.7636\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.451417\tTrain AUC: 0.9034\tVal AUC: 0.8128\n",
      "Epoch: [5/20]\tLoss: 0.396832\tTrain AUC: 0.9346\tVal AUC: 0.7849\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.341618\tTrain AUC: 0.9502\tVal AUC: 0.7830\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.296543\tTrain AUC: 0.9578\tVal AUC: 0.7790\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=7, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1672  460]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5195\tVal AUC: 0.5038\n",
      "Epoch: [1/20]\tLoss: 0.642118\tTrain AUC: 0.7394\tVal AUC: 0.7286\n",
      "Epoch: [2/20]\tLoss: 0.568030\tTrain AUC: 0.6770\tVal AUC: 0.6485\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.530680\tTrain AUC: 0.8238\tVal AUC: 0.7622\n",
      "Epoch: [4/20]\tLoss: 0.482014\tTrain AUC: 0.8280\tVal AUC: 0.7396\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.426644\tTrain AUC: 0.8626\tVal AUC: 0.7925\n",
      "Epoch: [6/20]\tLoss: 0.419847\tTrain AUC: 0.8827\tVal AUC: 0.7522\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.352261\tTrain AUC: 0.9273\tVal AUC: 0.7738\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.279572\tTrain AUC: 0.9668\tVal AUC: 0.8185\n",
      "Epoch: [9/20]\tLoss: 0.207855\tTrain AUC: 0.9769\tVal AUC: 0.8209\n",
      "Epoch: [10/20]\tLoss: 0.175877\tTrain AUC: 0.9854\tVal AUC: 0.8103\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=8, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1666  466]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5222\tVal AUC: 0.5048\n",
      "Epoch: [1/20]\tLoss: 0.604685\tTrain AUC: 0.7375\tVal AUC: 0.6427\n",
      "Epoch: [2/20]\tLoss: 0.499656\tTrain AUC: 0.8560\tVal AUC: 0.6409\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.453063\tTrain AUC: 0.8710\tVal AUC: 0.6969\n",
      "Epoch: [4/20]\tLoss: 0.431733\tTrain AUC: 0.8926\tVal AUC: 0.6991\n",
      "Epoch: [5/20]\tLoss: 0.411852\tTrain AUC: 0.9185\tVal AUC: 0.6927\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.332038\tTrain AUC: 0.9115\tVal AUC: 0.7209\n",
      "Epoch: [7/20]\tLoss: 0.333730\tTrain AUC: 0.9476\tVal AUC: 0.7350\n",
      "Epoch: [8/20]\tLoss: 0.327725\tTrain AUC: 0.9567\tVal AUC: 0.7282\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.261307\tTrain AUC: 0.9760\tVal AUC: 0.7677\n",
      "Epoch: [10/20]\tLoss: 0.224867\tTrain AUC: 0.9635\tVal AUC: 0.7515\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [11/20]\tLoss: 0.215478\tTrain AUC: 0.9860\tVal AUC: 0.7651\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=9, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2058, val:568\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1626  432]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5307\tVal AUC: 0.5139\n",
      "Epoch: [1/20]\tLoss: 0.662397\tTrain AUC: 0.7441\tVal AUC: 0.6840\n",
      "Epoch: [2/20]\tLoss: 0.546205\tTrain AUC: 0.7857\tVal AUC: 0.7293\n",
      "Epoch: [3/20]\tLoss: 0.488833\tTrain AUC: 0.8518\tVal AUC: 0.7439\n",
      "Epoch: [4/20]\tLoss: 0.456205\tTrain AUC: 0.8909\tVal AUC: 0.7692\n",
      "Epoch: [5/20]\tLoss: 0.462204\tTrain AUC: 0.8308\tVal AUC: 0.7217\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.371308\tTrain AUC: 0.9182\tVal AUC: 0.7133\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.295180\tTrain AUC: 0.9620\tVal AUC: 0.7553\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.232995\tTrain AUC: 0.9740\tVal AUC: 0.7479\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.200821\tTrain AUC: 0.9830\tVal AUC: 0.7662\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=10, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2048, val:578\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1644  404]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4737\tVal AUC: 0.4721\n",
      "Epoch: [1/20]\tLoss: 0.624286\tTrain AUC: 0.7881\tVal AUC: 0.7393\n",
      "Epoch: [2/20]\tLoss: 0.520049\tTrain AUC: 0.8122\tVal AUC: 0.7071\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.516510\tTrain AUC: 0.8340\tVal AUC: 0.6863\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.407689\tTrain AUC: 0.9267\tVal AUC: 0.7319\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.317930\tTrain AUC: 0.9549\tVal AUC: 0.7415\n",
      "Epoch: [6/20]\tLoss: 0.267470\tTrain AUC: 0.9685\tVal AUC: 0.7718\n",
      "Epoch: [7/20]\tLoss: 0.231202\tTrain AUC: 0.9820\tVal AUC: 0.7562\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.175350\tTrain AUC: 0.9901\tVal AUC: 0.7470\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 11 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=11, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2184, val:442\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1690  494]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5189\tVal AUC: 0.5048\n",
      "Epoch: [1/20]\tLoss: 0.608527\tTrain AUC: 0.7619\tVal AUC: 0.7312\n",
      "Epoch: [2/20]\tLoss: 0.533021\tTrain AUC: 0.6980\tVal AUC: 0.6444\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.522800\tTrain AUC: 0.8156\tVal AUC: 0.7042\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.412113\tTrain AUC: 0.9095\tVal AUC: 0.7859\n",
      "Epoch: [5/20]\tLoss: 0.379087\tTrain AUC: 0.9265\tVal AUC: 0.7728\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.334402\tTrain AUC: 0.9496\tVal AUC: 0.7757\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.288679\tTrain AUC: 0.9554\tVal AUC: 0.7766\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 12 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=12, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2096, val:530\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1628  468]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5494\tVal AUC: 0.5449\n",
      "Epoch: [1/20]\tLoss: 0.692905\tTrain AUC: 0.7185\tVal AUC: 0.6832\n",
      "Epoch: [2/20]\tLoss: 0.565976\tTrain AUC: 0.7695\tVal AUC: 0.6946\n",
      "Epoch: [3/20]\tLoss: 0.514974\tTrain AUC: 0.7935\tVal AUC: 0.6879\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.478437\tTrain AUC: 0.8681\tVal AUC: 0.7423\n",
      "Epoch: [5/20]\tLoss: 0.417090\tTrain AUC: 0.8925\tVal AUC: 0.7821\n",
      "Epoch: [6/20]\tLoss: 0.393715\tTrain AUC: 0.7850\tVal AUC: 0.6196\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.350529\tTrain AUC: 0.8165\tVal AUC: 0.6414\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.284539\tTrain AUC: 0.9712\tVal AUC: 0.7963\n",
      "Epoch: [9/20]\tLoss: 0.216712\tTrain AUC: 0.9813\tVal AUC: 0.7914\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.173316\tTrain AUC: 0.9858\tVal AUC: 0.7585\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 13 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=13, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2072, val:554\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1632  440]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5364\tVal AUC: 0.5321\n",
      "Epoch: [1/20]\tLoss: 0.618180\tTrain AUC: 0.6791\tVal AUC: 0.6084\n",
      "Epoch: [2/20]\tLoss: 0.543578\tTrain AUC: 0.7202\tVal AUC: 0.6781\n",
      "Epoch: [3/20]\tLoss: 0.474866\tTrain AUC: 0.8810\tVal AUC: 0.7865\n",
      "Epoch: [4/20]\tLoss: 0.444469\tTrain AUC: 0.8970\tVal AUC: 0.7811\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.393813\tTrain AUC: 0.8743\tVal AUC: 0.6697\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.329324\tTrain AUC: 0.9511\tVal AUC: 0.7872\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.255176\tTrain AUC: 0.9691\tVal AUC: 0.7780\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.228879\tTrain AUC: 0.9781\tVal AUC: 0.7618\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 14 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=14, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2162, val:464\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1688  474]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4820\tVal AUC: 0.4648\n",
      "Epoch: [1/20]\tLoss: 0.622725\tTrain AUC: 0.4521\tVal AUC: 0.4865\n",
      "Epoch: [2/20]\tLoss: 0.542439\tTrain AUC: 0.8134\tVal AUC: 0.8118\n",
      "Epoch: [3/20]\tLoss: 0.476384\tTrain AUC: 0.8515\tVal AUC: 0.8089\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.442797\tTrain AUC: 0.9006\tVal AUC: 0.8209\n",
      "Epoch: [5/20]\tLoss: 0.413406\tTrain AUC: 0.8242\tVal AUC: 0.7392\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.368837\tTrain AUC: 0.8147\tVal AUC: 0.7841\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.340739\tTrain AUC: 0.9611\tVal AUC: 0.8344\n",
      "Epoch: [8/20]\tLoss: 0.250705\tTrain AUC: 0.9727\tVal AUC: 0.8477\n",
      "Epoch: [9/20]\tLoss: 0.191854\tTrain AUC: 0.9802\tVal AUC: 0.8448\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.158534\tTrain AUC: 0.9887\tVal AUC: 0.8432\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 15 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=15, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2096, val:530\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1616  480]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4649\tVal AUC: 0.4731\n",
      "Epoch: [1/20]\tLoss: 0.656178\tTrain AUC: 0.6731\tVal AUC: 0.5483\n",
      "Epoch: [2/20]\tLoss: 0.534626\tTrain AUC: 0.7692\tVal AUC: 0.7053\n",
      "Epoch: [3/20]\tLoss: 0.507493\tTrain AUC: 0.6802\tVal AUC: 0.5168\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.498726\tTrain AUC: 0.8485\tVal AUC: 0.6643\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.408405\tTrain AUC: 0.9170\tVal AUC: 0.7078\n",
      "Epoch: [6/20]\tLoss: 0.369980\tTrain AUC: 0.9341\tVal AUC: 0.7324\n",
      "Epoch: [7/20]\tLoss: 0.322290\tTrain AUC: 0.9471\tVal AUC: 0.7083\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.298255\tTrain AUC: 0.9572\tVal AUC: 0.7216\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.273790\tTrain AUC: 0.9646\tVal AUC: 0.7344\n",
      "Epoch: [10/20]\tLoss: 0.245845\tTrain AUC: 0.9677\tVal AUC: 0.7352\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 16 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=16, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2086, val:540\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1662  424]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4508\tVal AUC: 0.5452\n",
      "Epoch: [1/20]\tLoss: 0.629642\tTrain AUC: 0.7755\tVal AUC: 0.7210\n",
      "Epoch: [2/20]\tLoss: 0.519240\tTrain AUC: 0.6094\tVal AUC: 0.6739\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.455880\tTrain AUC: 0.8437\tVal AUC: 0.7706\n",
      "Epoch: [4/20]\tLoss: 0.407419\tTrain AUC: 0.9158\tVal AUC: 0.7801\n",
      "Epoch: [5/20]\tLoss: 0.386764\tTrain AUC: 0.8858\tVal AUC: 0.7719\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.339724\tTrain AUC: 0.9400\tVal AUC: 0.7456\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.261819\tTrain AUC: 0.9785\tVal AUC: 0.7728\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.193877\tTrain AUC: 0.9897\tVal AUC: 0.7521\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 17 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=17, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2126, val:500\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1658  468]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5252\tVal AUC: 0.4867\n",
      "Epoch: [1/20]\tLoss: 0.656855\tTrain AUC: 0.7762\tVal AUC: 0.7025\n",
      "Epoch: [2/20]\tLoss: 0.541397\tTrain AUC: 0.7882\tVal AUC: 0.7227\n",
      "Epoch: [3/20]\tLoss: 0.507390\tTrain AUC: 0.8162\tVal AUC: 0.6962\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.489649\tTrain AUC: 0.7547\tVal AUC: 0.6519\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.417355\tTrain AUC: 0.9151\tVal AUC: 0.7673\n",
      "Epoch: [6/20]\tLoss: 0.348704\tTrain AUC: 0.9403\tVal AUC: 0.7523\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.334158\tTrain AUC: 0.9457\tVal AUC: 0.7572\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.277936\tTrain AUC: 0.9590\tVal AUC: 0.7672\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 18 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=18, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2138, val:488\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1682  456]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4768\tVal AUC: 0.4993\n",
      "Epoch: [1/20]\tLoss: 0.627797\tTrain AUC: 0.7973\tVal AUC: 0.7926\n",
      "Epoch: [2/20]\tLoss: 0.518404\tTrain AUC: 0.7681\tVal AUC: 0.7052\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.470397\tTrain AUC: 0.8789\tVal AUC: 0.7937\n",
      "Epoch: [4/20]\tLoss: 0.475202\tTrain AUC: 0.8760\tVal AUC: 0.7816\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.380722\tTrain AUC: 0.9274\tVal AUC: 0.8122\n",
      "Epoch: [6/20]\tLoss: 0.343399\tTrain AUC: 0.9200\tVal AUC: 0.7826\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.319215\tTrain AUC: 0.8879\tVal AUC: 0.7109\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.228263\tTrain AUC: 0.9876\tVal AUC: 0.8193\n",
      "Epoch: [9/20]\tLoss: 0.143662\tTrain AUC: 0.9937\tVal AUC: 0.8221\n",
      "Epoch: [10/20]\tLoss: 0.115245\tTrain AUC: 0.9957\tVal AUC: 0.8108\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 19 --run 7 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=19, run=7, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2078, val:548\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1628  450]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4774\tVal AUC: 0.5177\n",
      "Epoch: [1/20]\tLoss: 0.644814\tTrain AUC: 0.7924\tVal AUC: 0.7489\n",
      "Epoch: [2/20]\tLoss: 0.552709\tTrain AUC: 0.8120\tVal AUC: 0.7617\n",
      "Epoch: [3/20]\tLoss: 0.499354\tTrain AUC: 0.8577\tVal AUC: 0.7393\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.469881\tTrain AUC: 0.8708\tVal AUC: 0.7735\n",
      "Epoch: [5/20]\tLoss: 0.418550\tTrain AUC: 0.9252\tVal AUC: 0.7616\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.338359\tTrain AUC: 0.8663\tVal AUC: 0.6377\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.309514\tTrain AUC: 0.9644\tVal AUC: 0.7632\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.229006\tTrain AUC: 0.9792\tVal AUC: 0.7610\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# FROM SCRATCH MODELS - Parameter of the best split\n",
    "\n",
    "# value = 0.87421679\n",
    "# params_augm = 45\n",
    "# params_balance = True\n",
    "# params_batch_size = 32\n",
    "# params_cls_arch = simple\n",
    "# params_lr = 0.0007149374872274\n",
    "# params_lr_scheduler = True          -- AVEC CES PARAMETRE LA EN VERT CA A BIEN DONNE DE LA MERDE BIZARREMENT.\n",
    "# params_optimizer = adam\n",
    "# params_oversample = False\n",
    "# params_wd = 0.0004372932534312\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for split in range(20):\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 7 --nepochs 20 \"\n",
    "        f\"--lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler \"\n",
    "        f\"--optimizer adam --cls_arch simple --augm 4 --early_stopping --normalize\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "need to rety 6 with those parameters : \n",
    "\n",
    "The best result was achieved in the run with a value of 0.88161. For that run (number 14, split 3), the optimal training parameters were:\n",
    "\n",
    "• Balance: False\n",
    "• Batch Size: 16\n",
    "• Classifier Architecture: simple\n",
    "• Learning Rate: ~0.00444\n",
    "• LR Scheduler: Enabled (True)\n",
    "• Optimizer: SGD\n",
    "• Weight Decay: ~0.00241\n",
    "\n",
    "Since it used a simple classifier architecture, dropout and hidden dimension parameters are not applicable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch best parameter for each split - 10 - Nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 10 --nepochs 20 --lr 0.0005022148658806 --batch_size 128 --wd 0.006777245416484 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 14 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0005022148658806, lr_anneal=15, momentum=0.9, wd=0.006777245416484, split_index=0, run=10, batch_size=128, nepochs=20, workers=4, augm=14, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2078, val:548\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4861\tVal AUC: 0.4945\n",
      "Epoch: [1/20]\tLoss: 0.334220\tTrain AUC: 0.6931\tVal AUC: 0.6810\n",
      "Epoch: [2/20]\tLoss: 0.295482\tTrain AUC: 0.7740\tVal AUC: 0.7783\n",
      "Epoch: [3/20]\tLoss: 0.269175\tTrain AUC: 0.7567\tVal AUC: 0.7574\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.265086\tTrain AUC: 0.8084\tVal AUC: 0.7747\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.261262\tTrain AUC: 0.8657\tVal AUC: 0.7967\n",
      "Epoch: [6/20]\tLoss: 0.241114\tTrain AUC: 0.8727\tVal AUC: 0.7564\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.216863\tTrain AUC: 0.8991\tVal AUC: 0.7605\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.210354\tTrain AUC: 0.9027\tVal AUC: 0.7634\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 10 --nepochs 20 --lr 0.0007420484987937 --batch_size 32 --wd 0.000312513491152 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 3 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007420484987937, lr_anneal=15, momentum=0.9, wd=0.000312513491152, split_index=1, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=True, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2114, val:512\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5022\tVal AUC: 0.5375\n",
      "Epoch: [1/20]\tLoss: 0.347913\tTrain AUC: 0.6493\tVal AUC: 0.5076\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [2/20]\tLoss: 0.309495\tTrain AUC: 0.7997\tVal AUC: 0.6492\n",
      "Epoch: [3/20]\tLoss: 0.295549\tTrain AUC: 0.8159\tVal AUC: 0.6294\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.290559\tTrain AUC: 0.6476\tVal AUC: 0.5295\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.275602\tTrain AUC: 0.8166\tVal AUC: 0.5705\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.302185\tTrain AUC: 0.8092\tVal AUC: 0.5802\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 10 --nepochs 20 --lr 0.0002079532413834 --batch_size 16 --wd 0.0004568462804258 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0002079532413834, lr_anneal=15, momentum=0.9, wd=0.0004568462804258, split_index=2, run=10, batch_size=16, nepochs=20, workers=4, augm=34, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2118, val:508\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4650\tVal AUC: 0.5005\n",
      "Epoch: [1/20]\tLoss: 0.327293\tTrain AUC: 0.8083\tVal AUC: 0.7582\n",
      "Epoch: [2/20]\tLoss: 0.280774\tTrain AUC: 0.8481\tVal AUC: 0.7251\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.259914\tTrain AUC: 0.8962\tVal AUC: 0.7797\n",
      "Epoch: [4/20]\tLoss: 0.245538\tTrain AUC: 0.9151\tVal AUC: 0.8092\n",
      "Epoch: [5/20]\tLoss: 0.238298\tTrain AUC: 0.9212\tVal AUC: 0.8094\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.217825\tTrain AUC: 0.9006\tVal AUC: 0.8014\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.208495\tTrain AUC: 0.9022\tVal AUC: 0.7852\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.170835\tTrain AUC: 0.9710\tVal AUC: 0.8147\n",
      "Epoch: [9/20]\tLoss: 0.146418\tTrain AUC: 0.9772\tVal AUC: 0.8091\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 10 --nepochs 20 --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 45 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=3, run=10, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2164, val:462\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4536\tVal AUC: 0.4983\n",
      "Epoch: [1/20]\tLoss: 0.332223\tTrain AUC: 0.7567\tVal AUC: 0.7497\n",
      "Epoch: [2/20]\tLoss: 0.304376\tTrain AUC: 0.7998\tVal AUC: 0.7688\n",
      "Epoch: [3/20]\tLoss: 0.287114\tTrain AUC: 0.8116\tVal AUC: 0.7537\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.271309\tTrain AUC: 0.6614\tVal AUC: 0.5842\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.242083\tTrain AUC: 0.8901\tVal AUC: 0.7697\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.218674\tTrain AUC: 0.9063\tVal AUC: 0.7802\n",
      "Epoch: [7/20]\tLoss: 0.204726\tTrain AUC: 0.9266\tVal AUC: 0.7732\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.190429\tTrain AUC: 0.9351\tVal AUC: 0.7748\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 10 --nepochs 20 --lr 0.0097937719059157 --batch_size 64 --wd 0.0001142742366751 --optimizer sgd --cls_arch complex --dropout 0.4850920327988685 --hidden_dim 512 --balance  --early_stopping --augm 1 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=512, dropout=0.4850920327988685, optimizer='sgd', lr=0.0097937719059157, lr_anneal=15, momentum=0.9, wd=0.0001142742366751, split_index=4, run=10, batch_size=64, nepochs=20, workers=4, augm=1, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2106, val:520\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1664  442]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4793\tVal AUC: 0.4166\n",
      "Epoch: [1/20]\tLoss: 0.445848\tTrain AUC: 0.7372\tVal AUC: 0.7016\n",
      "Epoch: [2/20]\tLoss: 0.374289\tTrain AUC: 0.8043\tVal AUC: 0.7460\n",
      "Epoch: [3/20]\tLoss: 0.328644\tTrain AUC: 0.8596\tVal AUC: 0.7067\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.300042\tTrain AUC: 0.8846\tVal AUC: 0.7443\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.262522\tTrain AUC: 0.8918\tVal AUC: 0.7497\n",
      "Epoch: [6/20]\tLoss: 0.233780\tTrain AUC: 0.8864\tVal AUC: 0.7656\n",
      "Epoch: [7/20]\tLoss: 0.224208\tTrain AUC: 0.9302\tVal AUC: 0.7156\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.212592\tTrain AUC: 0.9041\tVal AUC: 0.7281\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.200488\tTrain AUC: 0.9609\tVal AUC: 0.6998\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 10 --nepochs 20 --lr 0.000219971883187 --batch_size 128 --wd 0.0053015787340158 --optimizer adam --cls_arch simple     --early_stopping --augm 3 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.000219971883187, lr_anneal=15, momentum=0.9, wd=0.0053015787340158, split_index=5, run=10, batch_size=128, nepochs=20, workers=4, augm=3, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5038\tVal AUC: 0.5498\n",
      "Epoch: [1/20]\tLoss: 0.471093\tTrain AUC: 0.7903\tVal AUC: 0.6723\n",
      "Epoch: [2/20]\tLoss: 0.351967\tTrain AUC: 0.8825\tVal AUC: 0.6745\n",
      "Epoch: [3/20]\tLoss: 0.308963\tTrain AUC: 0.9076\tVal AUC: 0.6747\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.273942\tTrain AUC: 0.9552\tVal AUC: 0.7291\n",
      "Epoch: [5/20]\tLoss: 0.216336\tTrain AUC: 0.9612\tVal AUC: 0.7379\n",
      "Epoch: [6/20]\tLoss: 0.195779\tTrain AUC: 0.9743\tVal AUC: 0.7269\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.171547\tTrain AUC: 0.9789\tVal AUC: 0.7465\n",
      "Epoch: [8/20]\tLoss: 0.158541\tTrain AUC: 0.9754\tVal AUC: 0.7569\n",
      "Epoch: [9/20]\tLoss: 0.147174\tTrain AUC: 0.9627\tVal AUC: 0.7543\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [10/20]\tLoss: 0.112552\tTrain AUC: 0.9908\tVal AUC: 0.7402\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [11/20]\tLoss: 0.127210\tTrain AUC: 0.9755\tVal AUC: 0.7110\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 10 --nepochs 20 --lr 0.000101887888748 --batch_size 16 --wd 0.0007386112995825 --optimizer adam --cls_arch simple    --lr_scheduler --early_stopping --augm 5 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.000101887888748, lr_anneal=15, momentum=0.9, wd=0.0007386112995825, split_index=6, run=10, batch_size=16, nepochs=20, workers=4, augm=5, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2110, val:516\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4995\tVal AUC: 0.5133\n",
      "Epoch: [1/20]\tLoss: 0.469135\tTrain AUC: 0.8449\tVal AUC: 0.7459\n",
      "Epoch: [2/20]\tLoss: 0.384205\tTrain AUC: 0.8868\tVal AUC: 0.7387\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.331919\tTrain AUC: 0.9283\tVal AUC: 0.7571\n",
      "Epoch: [4/20]\tLoss: 0.276470\tTrain AUC: 0.9562\tVal AUC: 0.8017\n",
      "Epoch: [5/20]\tLoss: 0.251184\tTrain AUC: 0.9641\tVal AUC: 0.7890\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.207009\tTrain AUC: 0.9840\tVal AUC: 0.7648\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.156860\tTrain AUC: 0.9965\tVal AUC: 0.7808\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.105615\tTrain AUC: 0.9980\tVal AUC: 0.7694\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 10 --nepochs 20 --lr 0.0002221924576152 --batch_size 16 --wd 0.000296459964069 --optimizer adam --cls_arch complex --dropout 0.1429591994388154 --hidden_dim 512  --lr_scheduler --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=512, dropout=0.1429591994388154, optimizer='adam', lr=0.0002221924576152, lr_anneal=15, momentum=0.9, wd=0.000296459964069, split_index=7, run=10, batch_size=16, nepochs=20, workers=4, augm=34, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5113\tVal AUC: 0.5343\n",
      "Epoch: [1/20]\tLoss: 0.471059\tTrain AUC: 0.7722\tVal AUC: 0.8194\n",
      "Epoch: [2/20]\tLoss: 0.434384\tTrain AUC: 0.8203\tVal AUC: 0.8126\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.407708\tTrain AUC: 0.8115\tVal AUC: 0.7970\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.361149\tTrain AUC: 0.8891\tVal AUC: 0.8445\n",
      "Epoch: [5/20]\tLoss: 0.330753\tTrain AUC: 0.9080\tVal AUC: 0.8517\n",
      "Epoch: [6/20]\tLoss: 0.312406\tTrain AUC: 0.9292\tVal AUC: 0.8356\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.284630\tTrain AUC: 0.9412\tVal AUC: 0.8519\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.271480\tTrain AUC: 0.9577\tVal AUC: 0.8607\n",
      "Epoch: [9/20]\tLoss: 0.254984\tTrain AUC: 0.9644\tVal AUC: 0.8538\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 10 --nepochs 20 --lr 0.0004081745903797 --batch_size 32 --wd 0.000339025460612 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 3 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0004081745903797, lr_anneal=15, momentum=0.9, wd=0.000339025460612, split_index=8, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2132, val:494\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1666  466]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5444\tVal AUC: 0.5268\n",
      "Epoch: [1/20]\tLoss: 0.377499\tTrain AUC: 0.8237\tVal AUC: 0.7033\n",
      "Epoch: [2/20]\tLoss: 0.338772\tTrain AUC: 0.8447\tVal AUC: 0.7102\n",
      "Epoch: [3/20]\tLoss: 0.275489\tTrain AUC: 0.8635\tVal AUC: 0.7437\n",
      "Epoch: [4/20]\tLoss: 0.265937\tTrain AUC: 0.9104\tVal AUC: 0.7069\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.240738\tTrain AUC: 0.8888\tVal AUC: 0.6554\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.212462\tTrain AUC: 0.9487\tVal AUC: 0.7444\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.201002\tTrain AUC: 0.9502\tVal AUC: 0.7483\n",
      "Epoch: [8/20]\tLoss: 0.189041\tTrain AUC: 0.9761\tVal AUC: 0.7231\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.164011\tTrain AUC: 0.9710\tVal AUC: 0.7302\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 10 --nepochs 20 --lr 0.0001005048871963 --batch_size 16 --wd 0.0001003076114388 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 5 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0001005048871963, lr_anneal=15, momentum=0.9, wd=0.0001003076114388, split_index=9, run=10, batch_size=16, nepochs=20, workers=4, augm=5, balance=True, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2058, val:568\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1626  432]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4418\tVal AUC: 0.4157\n",
      "Epoch: [1/20]\tLoss: 0.369177\tTrain AUC: 0.8334\tVal AUC: 0.7432\n",
      "Epoch: [2/20]\tLoss: 0.276941\tTrain AUC: 0.9413\tVal AUC: 0.7909\n",
      "Epoch: [3/20]\tLoss: 0.205329\tTrain AUC: 0.9512\tVal AUC: 0.7624\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.162620\tTrain AUC: 0.9722\tVal AUC: 0.7491\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.102136\tTrain AUC: 0.9951\tVal AUC: 0.7672\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.085587\tTrain AUC: 0.9973\tVal AUC: 0.7758\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.077038\tTrain AUC: 0.9982\tVal AUC: 0.7760\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 10 --nepochs 20 --lr 0.0001234129879286 --batch_size 64 --wd 0.0018556624076787 --optimizer adam --cls_arch complex --dropout 0.2338216995325822 --hidden_dim 320 --balance  --early_stopping --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=320, dropout=0.2338216995325822, optimizer='adam', lr=0.0001234129879286, lr_anneal=15, momentum=0.9, wd=0.0018556624076787, split_index=10, run=10, batch_size=64, nepochs=20, workers=4, augm=4, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2048, val:578\n",
      "Weight of each class, no tumor: 0.2078651685393258, tumor: 0.7921348314606742\n",
      "Balance loss with weights: [np.float64(0.2078651685393258), np.float64(0.7921348314606742)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1644  404]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4993\tVal AUC: 0.5247\n",
      "Epoch: [1/20]\tLoss: 0.367367\tTrain AUC: 0.8656\tVal AUC: 0.7394\n",
      "Epoch: [2/20]\tLoss: 0.219195\tTrain AUC: 0.8776\tVal AUC: 0.6508\n",
      "INFO: Early stopping counter 1 of 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters for training from scratch\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row[\"split\"])\n",
    "    \n",
    "    # Retrieve hyperparameters from CSV\n",
    "    lr = row[\"params_lr\"]\n",
    "    batch_size = row[\"params_batch_size\"]\n",
    "    wd = row[\"params_wd\"]\n",
    "    optimizer = row[\"params_optimizer\"]\n",
    "    cls_arch = row[\"params_cls_arch\"]  # e.g., 'simple' or 'complex'\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Dropout and hidden_dim flags\n",
    "    # ----------------------------\n",
    "    dropout_flag = \"\"\n",
    "    if \"params_dropout\" in row and not pd.isna(row[\"params_dropout\"]):\n",
    "        dropout_flag = f\"--dropout {row['params_dropout']}\"\n",
    "    \n",
    "    hidden_dim_flag = \"\"\n",
    "    if \"params_hidden_dim\" in row and not pd.isna(row[\"params_hidden_dim\"]):\n",
    "        hidden_dim_val = row[\"params_hidden_dim\"]\n",
    "        # If hidden_dim is non-integer but numeric, convert to int if appropriate\n",
    "        if not math.isnan(hidden_dim_val):\n",
    "            hidden_dim_flag = f\"--hidden_dim {int(hidden_dim_val)}\"\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Balance and LR scheduler\n",
    "    # ----------------------------\n",
    "    balance_flag = \"\"\n",
    "    if str(row[\"params_balance\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        balance_flag = \"--balance\"\n",
    "\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row[\"params_lr_scheduler\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # Parse params_augm for the --augm <val> option\n",
    "    # ---------------------------------------------\n",
    "    augm_flag = \"\"\n",
    "    if \"params_augm\" in row and not pd.isna(row[\"params_augm\"]):\n",
    "        augm_val = int(row[\"params_augm\"])  # ensure it's an integer\n",
    "        augm_flag = f\"--augm {augm_val}\"\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # Parse params_oversample for the --oversample\n",
    "    # ---------------------------------------------\n",
    "    oversample_flag = \"\"\n",
    "    if str(row[\"params_oversample\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        oversample_flag = \"--oversample\"\n",
    "    \n",
    "    # Build the command string\n",
    "    command = (\n",
    "        f\"python train.py \"\n",
    "        f\"--split_index {split} \"\n",
    "        f\"--run 10 \"\n",
    "        f\"--nepochs 20 \"\n",
    "        f\"--lr {lr} \"\n",
    "        f\"--batch_size {int(batch_size)} \"\n",
    "        f\"--wd {wd} \"\n",
    "        f\"--optimizer {optimizer} \"\n",
    "        f\"--cls_arch {cls_arch} \"      # Classification layer architecture\n",
    "        f\"{dropout_flag} \"            # e.g., --dropout 0.48\n",
    "        f\"{hidden_dim_flag} \"         # e.g., --hidden_dim 448\n",
    "        f\"{balance_flag} \"            # e.g., --balance\n",
    "        f\"{lr_scheduler_flag} \"       # e.g., --lr_scheduler\n",
    "        f\"--early_stopping \"\n",
    "        f\"{augm_flag} \"               # e.g., --augm 3\n",
    "        f\"{oversample_flag}\"          # e.g., --oversample\n",
    "    )\n",
    "    \n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
