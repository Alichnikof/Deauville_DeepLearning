{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to train using all the best parameters for each split (but using different models) 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters per split\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop over each row (each split) and build the command using the tuned parameters\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row['split'])\n",
    "    # Build checkpoint path as before\n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    \n",
    "    # Get hyperparameters from the CSV row.\n",
    "    lr = row['params_lr']              # e.g., 0.0000143417\n",
    "    batch_size = row['params_batch_size']  # e.g., 128\n",
    "    wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "    optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "    # Use the ft_mode field to decide which flag to pass:\n",
    "    ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "    ft_flag = \"\"\n",
    "    if ft_mode == \"finetune\":\n",
    "        ft_flag = \"--finetune\"\n",
    "    elif ft_mode == \"transfer_learning\":\n",
    "        ft_flag = \"--transfer_learning\"\n",
    "    # for full_retrain, no additional flag is needed\n",
    "    \n",
    "    # If lr_scheduler is True, add that flag.\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # If balance flag is True, add that flag.\n",
    "    balance_flag = \"\"\n",
    "    if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "        balance_flag = \"--balance\"\n",
    "    \n",
    "    # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Construct the command string\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 4 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "        f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "        f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar but just the same checkpoint, probably best approach. 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters per split\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop over each row (each split) and build the command using the tuned parameters\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row['split'])\n",
    "    # Build checkpoint path as before\n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth\"\n",
    "    \n",
    "    # Get hyperparameters from the CSV row.\n",
    "    lr = row['params_lr']              # e.g., 0.0000143417\n",
    "    batch_size = row['params_batch_size']  # e.g., 128\n",
    "    wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "    optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "    # Use the ft_mode field to decide which flag to pass:\n",
    "    ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "    ft_flag = \"\"\n",
    "    if ft_mode == \"finetune\":\n",
    "        ft_flag = \"--finetune\"\n",
    "    elif ft_mode == \"transfer_learning\":\n",
    "        ft_flag = \"--transfer_learning\"\n",
    "    # for full_retrain, no additional flag is needed\n",
    "    \n",
    "    # If lr_scheduler is True, add that flag.\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # If balance flag is True, add that flag.\n",
    "    balance_flag = \"\"\n",
    "    if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "        balance_flag = \"--balance\"\n",
    "    \n",
    "    # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Construct the command string\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 5 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "        f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "        f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different checkpoint modle but same parameter run 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=0, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8319\tVal AUC: 0.8400\n",
      "Epoch: [1/20]\tLoss: 0.795695\tTrain AUC: 0.8582\tVal AUC: 0.8424\n",
      "Epoch: [2/20]\tLoss: 0.477286\tTrain AUC: 0.8692\tVal AUC: 0.8315\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.373951\tTrain AUC: 0.8812\tVal AUC: 0.8279\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.346648\tTrain AUC: 0.8964\tVal AUC: 0.8303\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.328387\tTrain AUC: 0.9066\tVal AUC: 0.8337\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.304013\tTrain AUC: 0.9202\tVal AUC: 0.8415\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=1, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8337\tVal AUC: 0.8982\n",
      "Epoch: [1/20]\tLoss: 0.719204\tTrain AUC: 0.8563\tVal AUC: 0.8995\n",
      "Epoch: [2/20]\tLoss: 0.456692\tTrain AUC: 0.8495\tVal AUC: 0.8905\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.377079\tTrain AUC: 0.8715\tVal AUC: 0.8918\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.336437\tTrain AUC: 0.8971\tVal AUC: 0.9008\n",
      "Epoch: [5/20]\tLoss: 0.327745\tTrain AUC: 0.9122\tVal AUC: 0.9022\n",
      "Epoch: [6/20]\tLoss: 0.301314\tTrain AUC: 0.9254\tVal AUC: 0.8963\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.287536\tTrain AUC: 0.9360\tVal AUC: 0.9059\n",
      "Epoch: [8/20]\tLoss: 0.265555\tTrain AUC: 0.9477\tVal AUC: 0.9040\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.249257\tTrain AUC: 0.9565\tVal AUC: 0.9056\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=2, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8450\tVal AUC: 0.8129\n",
      "Epoch: [1/20]\tLoss: 0.748679\tTrain AUC: 0.8560\tVal AUC: 0.8359\n",
      "Epoch: [2/20]\tLoss: 0.432387\tTrain AUC: 0.8550\tVal AUC: 0.8170\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.369103\tTrain AUC: 0.8729\tVal AUC: 0.8110\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.322029\tTrain AUC: 0.8959\tVal AUC: 0.8268\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.314677\tTrain AUC: 0.9078\tVal AUC: 0.8238\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.284953\tTrain AUC: 0.9192\tVal AUC: 0.8297\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=3, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8302\tVal AUC: 0.8929\n",
      "Epoch: [1/20]\tLoss: 0.705121\tTrain AUC: 0.8588\tVal AUC: 0.8960\n",
      "Epoch: [2/20]\tLoss: 0.448639\tTrain AUC: 0.8661\tVal AUC: 0.8700\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.362379\tTrain AUC: 0.8818\tVal AUC: 0.8723\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.346205\tTrain AUC: 0.8972\tVal AUC: 0.8799\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.327637\tTrain AUC: 0.9099\tVal AUC: 0.8829\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.317779\tTrain AUC: 0.9213\tVal AUC: 0.8763\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=4, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8335\tVal AUC: 0.8436\n",
      "Epoch: [1/20]\tLoss: 0.755982\tTrain AUC: 0.8600\tVal AUC: 0.8476\n",
      "Epoch: [2/20]\tLoss: 0.497949\tTrain AUC: 0.8679\tVal AUC: 0.8172\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.371583\tTrain AUC: 0.8849\tVal AUC: 0.8184\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.335742\tTrain AUC: 0.8999\tVal AUC: 0.8248\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.307319\tTrain AUC: 0.9138\tVal AUC: 0.8274\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.300011\tTrain AUC: 0.9233\tVal AUC: 0.8330\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=5, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8515\tVal AUC: 0.8151\n",
      "Epoch: [1/20]\tLoss: 0.748418\tTrain AUC: 0.8714\tVal AUC: 0.8247\n",
      "Epoch: [2/20]\tLoss: 0.462196\tTrain AUC: 0.8738\tVal AUC: 0.8149\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.376638\tTrain AUC: 0.8882\tVal AUC: 0.8147\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.332610\tTrain AUC: 0.9069\tVal AUC: 0.8241\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.318271\tTrain AUC: 0.9207\tVal AUC: 0.8209\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.287086\tTrain AUC: 0.9343\tVal AUC: 0.8212\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=6, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8451\tVal AUC: 0.8293\n",
      "Epoch: [1/20]\tLoss: 0.729225\tTrain AUC: 0.8535\tVal AUC: 0.8456\n",
      "Epoch: [2/20]\tLoss: 0.439046\tTrain AUC: 0.8571\tVal AUC: 0.8328\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.376330\tTrain AUC: 0.8778\tVal AUC: 0.8311\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.342079\tTrain AUC: 0.8976\tVal AUC: 0.8360\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.322452\tTrain AUC: 0.9115\tVal AUC: 0.8347\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.307880\tTrain AUC: 0.9251\tVal AUC: 0.8315\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=7, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8471\tVal AUC: 0.7402\n",
      "Epoch: [1/20]\tLoss: 0.768954\tTrain AUC: 0.8692\tVal AUC: 0.7733\n",
      "Epoch: [2/20]\tLoss: 0.432007\tTrain AUC: 0.8796\tVal AUC: 0.7721\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.356598\tTrain AUC: 0.8907\tVal AUC: 0.7757\n",
      "Epoch: [4/20]\tLoss: 0.313837\tTrain AUC: 0.9025\tVal AUC: 0.7883\n",
      "Epoch: [5/20]\tLoss: 0.310866\tTrain AUC: 0.9132\tVal AUC: 0.7963\n",
      "Epoch: [6/20]\tLoss: 0.291899\tTrain AUC: 0.9213\tVal AUC: 0.8086\n",
      "Epoch: [7/20]\tLoss: 0.271407\tTrain AUC: 0.9295\tVal AUC: 0.8153\n",
      "Epoch: [8/20]\tLoss: 0.265773\tTrain AUC: 0.9407\tVal AUC: 0.8127\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [9/20]\tLoss: 0.244595\tTrain AUC: 0.9447\tVal AUC: 0.8241\n",
      "Epoch: [10/20]\tLoss: 0.243048\tTrain AUC: 0.9526\tVal AUC: 0.8201\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.234959\tTrain AUC: 0.9631\tVal AUC: 0.8158\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.204501\tTrain AUC: 0.9687\tVal AUC: 0.8134\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=8, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8398\tVal AUC: 0.8253\n",
      "Epoch: [1/20]\tLoss: 0.678181\tTrain AUC: 0.8626\tVal AUC: 0.8333\n",
      "Epoch: [2/20]\tLoss: 0.416904\tTrain AUC: 0.8693\tVal AUC: 0.8234\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.366375\tTrain AUC: 0.8897\tVal AUC: 0.8272\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.316840\tTrain AUC: 0.9099\tVal AUC: 0.8272\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.299458\tTrain AUC: 0.9218\tVal AUC: 0.8348\n",
      "Epoch: [6/20]\tLoss: 0.289612\tTrain AUC: 0.9331\tVal AUC: 0.8381\n",
      "Epoch: [7/20]\tLoss: 0.258733\tTrain AUC: 0.9443\tVal AUC: 0.8383\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.252017\tTrain AUC: 0.9544\tVal AUC: 0.8370\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=9, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8014\tVal AUC: 0.9042\n",
      "Epoch: [1/20]\tLoss: 0.783342\tTrain AUC: 0.8366\tVal AUC: 0.9206\n",
      "Epoch: [2/20]\tLoss: 0.507437\tTrain AUC: 0.8455\tVal AUC: 0.9169\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.397385\tTrain AUC: 0.8564\tVal AUC: 0.9064\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.360524\tTrain AUC: 0.8755\tVal AUC: 0.9083\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.339109\tTrain AUC: 0.8934\tVal AUC: 0.9191\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.322820\tTrain AUC: 0.9072\tVal AUC: 0.9214\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=10, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 11 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=11, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 12 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=12, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 13 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=13, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 14 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=14, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 15 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=15, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 16 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=16, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 17 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=17, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 18 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=18, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# FINE TUNED MODEL\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "for split in range(0, 10): \n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 12 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 20 - One checkpoint model loded and we use parameter of the best split AUC - basically what Haggstrom does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 20 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=0, run=20, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8319\tVal AUC: 0.8400\n",
      "Epoch: [1/20]\tLoss: 0.783166\tTrain AUC: 0.8583\tVal AUC: 0.8419\n",
      "Epoch: [2/20]\tLoss: 0.465037\tTrain AUC: 0.8676\tVal AUC: 0.8256\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.401630\tTrain AUC: 0.8774\tVal AUC: 0.8213\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.353409\tTrain AUC: 0.8942\tVal AUC: 0.8282\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.332947\tTrain AUC: 0.9083\tVal AUC: 0.8307\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.303861\tTrain AUC: 0.9214\tVal AUC: 0.8408\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Best parameter from optuna fine tuning\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "for split in range(0,20): \n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth\"\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 20 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch - run 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=0, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1536  412]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4990\tVal AUC: 0.5029\n",
      "Epoch: [1/20]\tLoss: 0.660246\tTrain AUC: 0.6843\tVal AUC: 0.6470\n",
      "Epoch: [2/20]\tLoss: 0.535453\tTrain AUC: 0.8308\tVal AUC: 0.7738\n",
      "Epoch: [3/20]\tLoss: 0.469450\tTrain AUC: 0.8558\tVal AUC: 0.7820\n",
      "Epoch: [4/20]\tLoss: 0.420316\tTrain AUC: 0.8299\tVal AUC: 0.6994\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.437765\tTrain AUC: 0.9301\tVal AUC: 0.7377\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.298763\tTrain AUC: 0.9591\tVal AUC: 0.7738\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.282108\tTrain AUC: 0.9768\tVal AUC: 0.7515\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.194704\tTrain AUC: 0.9831\tVal AUC: 0.7868\n",
      "Epoch: [9/20]\tLoss: 0.155622\tTrain AUC: 0.9831\tVal AUC: 0.7928\n",
      "Epoch: [10/20]\tLoss: 0.132341\tTrain AUC: 0.9909\tVal AUC: 0.7852\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=1, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1578  436]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4408\tVal AUC: 0.4152\n",
      "Epoch: [1/20]\tLoss: 0.613683\tTrain AUC: 0.7525\tVal AUC: 0.7362\n",
      "Epoch: [2/20]\tLoss: 0.505517\tTrain AUC: 0.8588\tVal AUC: 0.8238\n",
      "Epoch: [3/20]\tLoss: 0.442865\tTrain AUC: 0.8261\tVal AUC: 0.7129\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.400313\tTrain AUC: 0.8537\tVal AUC: 0.7453\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.306078\tTrain AUC: 0.9667\tVal AUC: 0.8377\n",
      "Epoch: [6/20]\tLoss: 0.237853\tTrain AUC: 0.9731\tVal AUC: 0.8409\n",
      "Epoch: [7/20]\tLoss: 0.190656\tTrain AUC: 0.9856\tVal AUC: 0.8219\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.166337\tTrain AUC: 0.9915\tVal AUC: 0.8012\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.112347\tTrain AUC: 0.9943\tVal AUC: 0.8174\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=2, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1572  392]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5691\tVal AUC: 0.5213\n",
      "Epoch: [1/20]\tLoss: 0.592767\tTrain AUC: 0.6410\tVal AUC: 0.5963\n",
      "Epoch: [2/20]\tLoss: 0.466768\tTrain AUC: 0.8758\tVal AUC: 0.7707\n",
      "Epoch: [3/20]\tLoss: 0.422492\tTrain AUC: 0.9155\tVal AUC: 0.7388\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.377067\tTrain AUC: 0.9109\tVal AUC: 0.7340\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.310008\tTrain AUC: 0.9636\tVal AUC: 0.7475\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.241622\tTrain AUC: 0.9774\tVal AUC: 0.7270\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.178372\tTrain AUC: 0.9851\tVal AUC: 0.7634\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=3, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1534  448]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4761\tVal AUC: 0.4569\n",
      "Epoch: [1/20]\tLoss: 0.636020\tTrain AUC: 0.7806\tVal AUC: 0.7405\n",
      "Epoch: [2/20]\tLoss: 0.541722\tTrain AUC: 0.8168\tVal AUC: 0.8106\n",
      "Epoch: [3/20]\tLoss: 0.462494\tTrain AUC: 0.8644\tVal AUC: 0.8556\n",
      "Epoch: [4/20]\tLoss: 0.410891\tTrain AUC: 0.8732\tVal AUC: 0.7573\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.390792\tTrain AUC: 0.9154\tVal AUC: 0.8270\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.307658\tTrain AUC: 0.9662\tVal AUC: 0.8453\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.224327\tTrain AUC: 0.9766\tVal AUC: 0.8457\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.200631\tTrain AUC: 0.9869\tVal AUC: 0.8408\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=4, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1528  420]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5037\tVal AUC: 0.5156\n",
      "Epoch: [1/20]\tLoss: 0.636075\tTrain AUC: 0.5948\tVal AUC: 0.6082\n",
      "Epoch: [2/20]\tLoss: 0.532932\tTrain AUC: 0.7942\tVal AUC: 0.7060\n",
      "Epoch: [3/20]\tLoss: 0.479219\tTrain AUC: 0.8358\tVal AUC: 0.7504\n",
      "Epoch: [4/20]\tLoss: 0.456633\tTrain AUC: 0.8332\tVal AUC: 0.7281\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.396068\tTrain AUC: 0.8993\tVal AUC: 0.7378\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.330483\tTrain AUC: 0.9503\tVal AUC: 0.7737\n",
      "Epoch: [7/20]\tLoss: 0.250793\tTrain AUC: 0.9740\tVal AUC: 0.7791\n",
      "Epoch: [8/20]\tLoss: 0.198753\tTrain AUC: 0.9834\tVal AUC: 0.7847\n",
      "Epoch: [9/20]\tLoss: 0.173603\tTrain AUC: 0.9890\tVal AUC: 0.8005\n",
      "Epoch: [10/20]\tLoss: 0.135876\tTrain AUC: 0.9920\tVal AUC: 0.7809\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.081537\tTrain AUC: 0.9936\tVal AUC: 0.7871\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.077722\tTrain AUC: 0.9966\tVal AUC: 0.7957\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=5, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1552  424]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5298\tVal AUC: 0.4762\n",
      "Epoch: [1/20]\tLoss: 0.625531\tTrain AUC: 0.8092\tVal AUC: 0.7053\n",
      "Epoch: [2/20]\tLoss: 0.522408\tTrain AUC: 0.8323\tVal AUC: 0.7208\n",
      "Epoch: [3/20]\tLoss: 0.450233\tTrain AUC: 0.8554\tVal AUC: 0.7140\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.420606\tTrain AUC: 0.9080\tVal AUC: 0.7505\n",
      "Epoch: [5/20]\tLoss: 0.367909\tTrain AUC: 0.8918\tVal AUC: 0.7337\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.323844\tTrain AUC: 0.8961\tVal AUC: 0.7593\n",
      "Epoch: [7/20]\tLoss: 0.321332\tTrain AUC: 0.9180\tVal AUC: 0.6941\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.278447\tTrain AUC: 0.9649\tVal AUC: 0.7394\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.176538\tTrain AUC: 0.9910\tVal AUC: 0.7793\n",
      "Epoch: [10/20]\tLoss: 0.114085\tTrain AUC: 0.9953\tVal AUC: 0.7729\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=6, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1548  446]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4417\tVal AUC: 0.5218\n",
      "Epoch: [1/20]\tLoss: 0.633162\tTrain AUC: 0.7320\tVal AUC: 0.7227\n",
      "Epoch: [2/20]\tLoss: 0.548962\tTrain AUC: 0.8253\tVal AUC: 0.7908\n",
      "Epoch: [3/20]\tLoss: 0.502096\tTrain AUC: 0.8134\tVal AUC: 0.7227\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.441330\tTrain AUC: 0.7861\tVal AUC: 0.6855\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.435815\tTrain AUC: 0.9345\tVal AUC: 0.8147\n",
      "Epoch: [6/20]\tLoss: 0.337939\tTrain AUC: 0.9545\tVal AUC: 0.8157\n",
      "Epoch: [7/20]\tLoss: 0.268582\tTrain AUC: 0.9580\tVal AUC: 0.8017\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.250354\tTrain AUC: 0.9712\tVal AUC: 0.8031\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.217689\tTrain AUC: 0.9796\tVal AUC: 0.8018\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=7, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1596  410]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5133\tVal AUC: 0.5029\n",
      "Epoch: [1/20]\tLoss: 0.644530\tTrain AUC: 0.7346\tVal AUC: 0.6209\n",
      "Epoch: [2/20]\tLoss: 0.518785\tTrain AUC: 0.7453\tVal AUC: 0.7123\n",
      "Epoch: [3/20]\tLoss: 0.493655\tTrain AUC: 0.8610\tVal AUC: 0.7581\n",
      "Epoch: [4/20]\tLoss: 0.423748\tTrain AUC: 0.8930\tVal AUC: 0.7671\n",
      "Epoch: [5/20]\tLoss: 0.405121\tTrain AUC: 0.9234\tVal AUC: 0.7324\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.325236\tTrain AUC: 0.9039\tVal AUC: 0.7285\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.309364\tTrain AUC: 0.9650\tVal AUC: 0.7595\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.244726\tTrain AUC: 0.9783\tVal AUC: 0.7552\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.185433\tTrain AUC: 0.9869\tVal AUC: 0.7647\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=8, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1564  410]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5258\tVal AUC: 0.5120\n",
      "Epoch: [1/20]\tLoss: 0.629366\tTrain AUC: 0.7488\tVal AUC: 0.6448\n",
      "Epoch: [2/20]\tLoss: 0.505593\tTrain AUC: 0.8290\tVal AUC: 0.6917\n",
      "Epoch: [3/20]\tLoss: 0.454919\tTrain AUC: 0.8616\tVal AUC: 0.7383\n",
      "Epoch: [4/20]\tLoss: 0.408379\tTrain AUC: 0.9108\tVal AUC: 0.7749\n",
      "Epoch: [5/20]\tLoss: 0.360621\tTrain AUC: 0.8448\tVal AUC: 0.6484\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.320322\tTrain AUC: 0.9413\tVal AUC: 0.7627\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.255537\tTrain AUC: 0.9810\tVal AUC: 0.7491\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.191901\tTrain AUC: 0.9873\tVal AUC: 0.7715\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.142556\tTrain AUC: 0.9926\tVal AUC: 0.7540\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=9, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1576  416]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4811\tVal AUC: 0.4646\n",
      "Epoch: [1/20]\tLoss: 0.611483\tTrain AUC: 0.6760\tVal AUC: 0.6964\n",
      "Epoch: [2/20]\tLoss: 0.502089\tTrain AUC: 0.7772\tVal AUC: 0.6816\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.496864\tTrain AUC: 0.8262\tVal AUC: 0.8039\n",
      "Epoch: [4/20]\tLoss: 0.453579\tTrain AUC: 0.8808\tVal AUC: 0.8070\n",
      "Epoch: [5/20]\tLoss: 0.435892\tTrain AUC: 0.8374\tVal AUC: 0.7378\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.377718\tTrain AUC: 0.8484\tVal AUC: 0.7913\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.341626\tTrain AUC: 0.9635\tVal AUC: 0.8831\n",
      "Epoch: [8/20]\tLoss: 0.259626\tTrain AUC: 0.9775\tVal AUC: 0.8625\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.197897\tTrain AUC: 0.9840\tVal AUC: 0.8651\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=10, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1970, val:508\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1584  386]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5121\tVal AUC: 0.5311\n",
      "Epoch: [1/20]\tLoss: 0.622197\tTrain AUC: 0.8272\tVal AUC: 0.7209\n",
      "Epoch: [2/20]\tLoss: 0.469006\tTrain AUC: 0.8353\tVal AUC: 0.7739\n",
      "Epoch: [3/20]\tLoss: 0.448980\tTrain AUC: 0.8358\tVal AUC: 0.6802\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.411833\tTrain AUC: 0.9132\tVal AUC: 0.7759\n",
      "Epoch: [5/20]\tLoss: 0.363203\tTrain AUC: 0.9037\tVal AUC: 0.7958\n",
      "Epoch: [6/20]\tLoss: 0.317544\tTrain AUC: 0.8714\tVal AUC: 0.7324\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.307576\tTrain AUC: 0.9285\tVal AUC: 0.7356\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.217745\tTrain AUC: 0.9830\tVal AUC: 0.8025\n",
      "Epoch: [9/20]\tLoss: 0.155009\tTrain AUC: 0.9901\tVal AUC: 0.7920\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.110008\tTrain AUC: 0.9948\tVal AUC: 0.8051\n",
      "Epoch: [11/20]\tLoss: 0.098994\tTrain AUC: 0.9950\tVal AUC: 0.7912\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 11 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=11, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1960, val:518\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1554  406]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5223\tVal AUC: 0.5026\n",
      "Epoch: [1/20]\tLoss: 0.587924\tTrain AUC: 0.7803\tVal AUC: 0.6357\n",
      "Epoch: [2/20]\tLoss: 0.485747\tTrain AUC: 0.8333\tVal AUC: 0.6649\n",
      "Epoch: [3/20]\tLoss: 0.434641\tTrain AUC: 0.9021\tVal AUC: 0.6875\n",
      "Epoch: [4/20]\tLoss: 0.378905\tTrain AUC: 0.8819\tVal AUC: 0.6822\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.335957\tTrain AUC: 0.9388\tVal AUC: 0.7295\n",
      "Epoch: [6/20]\tLoss: 0.302758\tTrain AUC: 0.9500\tVal AUC: 0.7254\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.227496\tTrain AUC: 0.9766\tVal AUC: 0.7580\n",
      "Epoch: [8/20]\tLoss: 0.229476\tTrain AUC: 0.9755\tVal AUC: 0.6848\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.200088\tTrain AUC: 0.9793\tVal AUC: 0.7363\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.121088\tTrain AUC: 0.9971\tVal AUC: 0.7235\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 12 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=12, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1582  422]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5089\tVal AUC: 0.5229\n",
      "Epoch: [1/20]\tLoss: 0.637461\tTrain AUC: 0.7730\tVal AUC: 0.7123\n",
      "Epoch: [2/20]\tLoss: 0.504947\tTrain AUC: 0.8180\tVal AUC: 0.6726\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.485815\tTrain AUC: 0.7965\tVal AUC: 0.6872\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.440784\tTrain AUC: 0.9192\tVal AUC: 0.7437\n",
      "Epoch: [5/20]\tLoss: 0.338198\tTrain AUC: 0.9469\tVal AUC: 0.7325\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.305730\tTrain AUC: 0.9570\tVal AUC: 0.7589\n",
      "Epoch: [7/20]\tLoss: 0.258195\tTrain AUC: 0.9766\tVal AUC: 0.7595\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.197678\tTrain AUC: 0.9780\tVal AUC: 0.7624\n",
      "Epoch: [9/20]\tLoss: 0.191392\tTrain AUC: 0.9863\tVal AUC: 0.7530\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 13 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=13, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1944, val:534\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1566  378]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4679\tVal AUC: 0.4611\n",
      "Epoch: [1/20]\tLoss: 0.609684\tTrain AUC: 0.7636\tVal AUC: 0.7206\n",
      "Epoch: [2/20]\tLoss: 0.471440\tTrain AUC: 0.8083\tVal AUC: 0.7112\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.473091\tTrain AUC: 0.7266\tVal AUC: 0.6520\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.426204\tTrain AUC: 0.9088\tVal AUC: 0.7678\n",
      "Epoch: [5/20]\tLoss: 0.358908\tTrain AUC: 0.9364\tVal AUC: 0.7726\n",
      "Epoch: [6/20]\tLoss: 0.317828\tTrain AUC: 0.9546\tVal AUC: 0.7620\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.232331\tTrain AUC: 0.9707\tVal AUC: 0.7293\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.234558\tTrain AUC: 0.9771\tVal AUC: 0.7481\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 14 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=14, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2038, val:440\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1618  420]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5313\tVal AUC: 0.5285\n",
      "Epoch: [1/20]\tLoss: 0.626747\tTrain AUC: 0.6524\tVal AUC: 0.6874\n",
      "Epoch: [2/20]\tLoss: 0.529170\tTrain AUC: 0.7759\tVal AUC: 0.7273\n",
      "Epoch: [3/20]\tLoss: 0.464573\tTrain AUC: 0.8757\tVal AUC: 0.7535\n",
      "Epoch: [4/20]\tLoss: 0.429672\tTrain AUC: 0.7881\tVal AUC: 0.7071\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.374591\tTrain AUC: 0.8960\tVal AUC: 0.7901\n",
      "Epoch: [6/20]\tLoss: 0.305931\tTrain AUC: 0.9364\tVal AUC: 0.7342\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.316605\tTrain AUC: 0.8552\tVal AUC: 0.6585\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.261488\tTrain AUC: 0.9780\tVal AUC: 0.8030\n",
      "Epoch: [9/20]\tLoss: 0.177736\tTrain AUC: 0.9871\tVal AUC: 0.7949\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.151339\tTrain AUC: 0.9922\tVal AUC: 0.7825\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 15 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=15, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1566  410]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4490\tVal AUC: 0.4887\n",
      "Epoch: [1/20]\tLoss: 0.574521\tTrain AUC: 0.8639\tVal AUC: 0.6881\n",
      "Epoch: [2/20]\tLoss: 0.470988\tTrain AUC: 0.8346\tVal AUC: 0.6934\n",
      "Epoch: [3/20]\tLoss: 0.425342\tTrain AUC: 0.8620\tVal AUC: 0.6621\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.386893\tTrain AUC: 0.9208\tVal AUC: 0.7120\n",
      "Epoch: [5/20]\tLoss: 0.335419\tTrain AUC: 0.9274\tVal AUC: 0.6695\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.294325\tTrain AUC: 0.9395\tVal AUC: 0.7059\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.218922\tTrain AUC: 0.9871\tVal AUC: 0.7024\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.127244\tTrain AUC: 0.9932\tVal AUC: 0.7029\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 16 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=16, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1844, val:634\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1446  398]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5306\tVal AUC: 0.5422\n",
      "Epoch: [1/20]\tLoss: 0.643451\tTrain AUC: 0.7192\tVal AUC: 0.6243\n",
      "Epoch: [2/20]\tLoss: 0.546558\tTrain AUC: 0.8265\tVal AUC: 0.7908\n",
      "Epoch: [3/20]\tLoss: 0.476868\tTrain AUC: 0.7858\tVal AUC: 0.6988\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.438164\tTrain AUC: 0.8629\tVal AUC: 0.8349\n",
      "Epoch: [5/20]\tLoss: 0.422438\tTrain AUC: 0.9252\tVal AUC: 0.8172\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.391961\tTrain AUC: 0.9253\tVal AUC: 0.7835\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.288337\tTrain AUC: 0.9621\tVal AUC: 0.8273\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.235350\tTrain AUC: 0.9798\tVal AUC: 0.8384\n",
      "Epoch: [9/20]\tLoss: 0.194426\tTrain AUC: 0.9847\tVal AUC: 0.8253\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 17 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=17, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1578  426]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5536\tVal AUC: 0.5367\n",
      "Epoch: [1/20]\tLoss: 0.594773\tTrain AUC: 0.8141\tVal AUC: 0.6679\n",
      "Epoch: [2/20]\tLoss: 0.492235\tTrain AUC: 0.8562\tVal AUC: 0.7458\n",
      "Epoch: [3/20]\tLoss: 0.465745\tTrain AUC: 0.8821\tVal AUC: 0.7356\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.419214\tTrain AUC: 0.7997\tVal AUC: 0.6664\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.336904\tTrain AUC: 0.9597\tVal AUC: 0.7245\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.278874\tTrain AUC: 0.9782\tVal AUC: 0.7587\n",
      "Epoch: [7/20]\tLoss: 0.199920\tTrain AUC: 0.9842\tVal AUC: 0.7054\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.156275\tTrain AUC: 0.9900\tVal AUC: 0.6867\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 18 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=18, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2030, val:448\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1588  442]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4689\tVal AUC: 0.5470\n",
      "Epoch: [1/20]\tLoss: 0.589107\tTrain AUC: 0.8196\tVal AUC: 0.7166\n",
      "Epoch: [2/20]\tLoss: 0.511927\tTrain AUC: 0.8046\tVal AUC: 0.6742\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.462244\tTrain AUC: 0.8626\tVal AUC: 0.7566\n",
      "Epoch: [4/20]\tLoss: 0.392867\tTrain AUC: 0.8947\tVal AUC: 0.7431\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.390470\tTrain AUC: 0.9200\tVal AUC: 0.6427\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.285238\tTrain AUC: 0.9701\tVal AUC: 0.7489\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.203809\tTrain AUC: 0.9825\tVal AUC: 0.7296\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 19 --run 6 --nepochs 20 --lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler --optimizer adam --cls_arch simple --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.00104, lr_anneal=15, momentum=0.9, wd=0.000138, split_index=19, run=6, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1938, val:540\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1506  432]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5244\tVal AUC: 0.5321\n",
      "Epoch: [1/20]\tLoss: 0.613600\tTrain AUC: 0.8208\tVal AUC: 0.7474\n",
      "Epoch: [2/20]\tLoss: 0.494667\tTrain AUC: 0.8233\tVal AUC: 0.7208\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.458234\tTrain AUC: 0.7100\tVal AUC: 0.4209\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.371817\tTrain AUC: 0.9442\tVal AUC: 0.7145\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.316844\tTrain AUC: 0.9619\tVal AUC: 0.7110\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.249989\tTrain AUC: 0.9711\tVal AUC: 0.6968\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# FROM SCRATCH MODELS\n",
    "\n",
    "\n",
    "# Optimal configuration based on your optimization results:\n",
    "# Learning Rate: 0.00104\n",
    "# Batch Size: 64\n",
    "# Optimizer: Adam\n",
    "# Weight Decay: 0.000138\n",
    "# Learning Rate Scheduler: Enabled\n",
    "# Balance Loss: Enabled\n",
    "# Classifier Architecture: simple\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for split in range(20):\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 6 --nepochs 20 \"\n",
    "        f\"--lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTUALLY WE SHOULD PROBABLY RE RUN OPTUN.PY cause now oversampling and augmentation included\n",
    "\n",
    "need to rety 6 with those parameters : \n",
    "\n",
    "The best result was achieved in the run with a value of 0.88161. For that run (number 14, split 3), the optimal training parameters were:\n",
    "\n",
    "• Balance: False\n",
    "• Batch Size: 16\n",
    "• Classifier Architecture: simple\n",
    "• Learning Rate: ~0.00444\n",
    "• LR Scheduler: Enabled (True)\n",
    "• Optimizer: SGD\n",
    "• Weight Decay: ~0.00241\n",
    "\n",
    "Since it used a simple classifier architecture, dropout and hidden dimension parameters are not applicable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch best parameter for each split - 10 - Nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 10 --nepochs 20 --lr 0.0004194869845244 --batch_size 128 --wd 0.0001030377652283 --optimizer adam --cls_arch complex --dropout 0.4851196771362177 --hidden_dim 448   --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=448, dropout=0.4851196771362177, optimizer='adam', lr=0.0004194869845244, lr_anneal=15, momentum=0.9, wd=0.0001030377652283, split_index=0, run=10, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5100\tVal AUC: 0.5164\n",
      "Epoch: [1/20]\tLoss: 0.486573\tTrain AUC: 0.7784\tVal AUC: 0.7786\n",
      "Epoch: [2/20]\tLoss: 0.392271\tTrain AUC: 0.8084\tVal AUC: 0.7121\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.328564\tTrain AUC: 0.9185\tVal AUC: 0.8107\n",
      "Epoch: [4/20]\tLoss: 0.271994\tTrain AUC: 0.8804\tVal AUC: 0.7604\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.210914\tTrain AUC: 0.9731\tVal AUC: 0.7918\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.186515\tTrain AUC: 0.9729\tVal AUC: 0.7224\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.156740\tTrain AUC: 0.8832\tVal AUC: 0.6883\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 10 --nepochs 20 --lr 0.0001534890452855 --batch_size 16 --wd 0.0005286954291243 --optimizer adam --cls_arch simple     --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0001534890452855, lr_anneal=15, momentum=0.9, wd=0.0005286954291243, split_index=1, run=10, batch_size=16, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5144\tVal AUC: 0.5422\n",
      "Epoch: [1/20]\tLoss: 0.464875\tTrain AUC: 0.8016\tVal AUC: 0.8015\n",
      "Epoch: [2/20]\tLoss: 0.402235\tTrain AUC: 0.9017\tVal AUC: 0.8377\n",
      "Epoch: [3/20]\tLoss: 0.349278\tTrain AUC: 0.8959\tVal AUC: 0.7975\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.307180\tTrain AUC: 0.9615\tVal AUC: 0.7977\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.269639\tTrain AUC: 0.9361\tVal AUC: 0.7602\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.250188\tTrain AUC: 0.9737\tVal AUC: 0.8186\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.214342\tTrain AUC: 0.9610\tVal AUC: 0.7695\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 10 --nepochs 20 --lr 0.0002399645692472 --batch_size 64 --wd 0.0017267986771503 --optimizer adam --cls_arch complex --dropout 0.1876483314663505 --hidden_dim 448  --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=448, dropout=0.1876483314663505, optimizer='adam', lr=0.0002399645692472, lr_anneal=15, momentum=0.9, wd=0.0017267986771503, split_index=2, run=10, batch_size=64, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5044\tVal AUC: 0.5054\n",
      "Epoch: [1/20]\tLoss: 0.451798\tTrain AUC: 0.8248\tVal AUC: 0.7051\n",
      "Epoch: [2/20]\tLoss: 0.352247\tTrain AUC: 0.8791\tVal AUC: 0.7100\n",
      "Epoch: [3/20]\tLoss: 0.302450\tTrain AUC: 0.9193\tVal AUC: 0.7476\n",
      "Epoch: [4/20]\tLoss: 0.245730\tTrain AUC: 0.9130\tVal AUC: 0.7614\n",
      "Epoch: [5/20]\tLoss: 0.256158\tTrain AUC: 0.9842\tVal AUC: 0.7527\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.166663\tTrain AUC: 0.9478\tVal AUC: 0.7565\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.107347\tTrain AUC: 0.9967\tVal AUC: 0.7918\n",
      "Epoch: [8/20]\tLoss: 0.076244\tTrain AUC: 0.9989\tVal AUC: 0.7877\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.048558\tTrain AUC: 0.9993\tVal AUC: 0.7914\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.047038\tTrain AUC: 0.9995\tVal AUC: 0.7913\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 10 --nepochs 20 --lr 0.0044413525595453 --batch_size 16 --wd 0.0024076079618982 --optimizer sgd --cls_arch simple    --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='sgd', lr=0.0044413525595453, lr_anneal=15, momentum=0.9, wd=0.0024076079618982, split_index=3, run=10, batch_size=16, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5230\tVal AUC: 0.5242\n",
      "Epoch: [1/20]\tLoss: 0.494933\tTrain AUC: 0.8167\tVal AUC: 0.7935\n",
      "Epoch: [2/20]\tLoss: 0.431691\tTrain AUC: 0.8045\tVal AUC: 0.7416\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.409694\tTrain AUC: 0.8830\tVal AUC: 0.8066\n",
      "Epoch: [4/20]\tLoss: 0.371018\tTrain AUC: 0.8735\tVal AUC: 0.7535\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.357411\tTrain AUC: 0.9155\tVal AUC: 0.7904\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.304487\tTrain AUC: 0.9196\tVal AUC: 0.8553\n",
      "Epoch: [7/20]\tLoss: 0.292766\tTrain AUC: 0.9631\tVal AUC: 0.7967\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.231660\tTrain AUC: 0.9709\tVal AUC: 0.7765\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 10 --nepochs 20 --lr 0.0002686823779312 --batch_size 32 --wd 0.0001273895275569 --optimizer adam --cls_arch simple     --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0002686823779312, lr_anneal=15, momentum=0.9, wd=0.0001273895275569, split_index=4, run=10, batch_size=32, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5266\tVal AUC: 0.5172\n",
      "Epoch: [1/20]\tLoss: 0.458717\tTrain AUC: 0.8226\tVal AUC: 0.7652\n",
      "Epoch: [2/20]\tLoss: 0.402600\tTrain AUC: 0.8213\tVal AUC: 0.7404\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.356195\tTrain AUC: 0.8815\tVal AUC: 0.7524\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.306947\tTrain AUC: 0.9125\tVal AUC: 0.7826\n",
      "Epoch: [5/20]\tLoss: 0.279988\tTrain AUC: 0.9694\tVal AUC: 0.7621\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.215312\tTrain AUC: 0.9555\tVal AUC: 0.7138\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.204600\tTrain AUC: 0.9865\tVal AUC: 0.7603\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 10 --nepochs 20 --lr 0.0002454684716003 --batch_size 32 --wd 0.0098957584208144 --optimizer adam --cls_arch complex --dropout 0.235275127981855 --hidden_dim 320  --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=320, dropout=0.235275127981855, optimizer='adam', lr=0.0002454684716003, lr_anneal=15, momentum=0.9, wd=0.0098957584208144, split_index=5, run=10, batch_size=32, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4838\tVal AUC: 0.5012\n",
      "Epoch: [1/20]\tLoss: 0.480471\tTrain AUC: 0.8027\tVal AUC: 0.7144\n",
      "Epoch: [2/20]\tLoss: 0.405873\tTrain AUC: 0.8658\tVal AUC: 0.7307\n",
      "Epoch: [3/20]\tLoss: 0.371275\tTrain AUC: 0.8912\tVal AUC: 0.7535\n",
      "Epoch: [4/20]\tLoss: 0.361468\tTrain AUC: 0.8443\tVal AUC: 0.6595\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.345547\tTrain AUC: 0.8595\tVal AUC: 0.7272\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.257845\tTrain AUC: 0.9626\tVal AUC: 0.7797\n",
      "Epoch: [7/20]\tLoss: 0.192181\tTrain AUC: 0.9744\tVal AUC: 0.7409\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.165230\tTrain AUC: 0.9859\tVal AUC: 0.7446\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.122916\tTrain AUC: 0.9904\tVal AUC: 0.7579\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 10 --nepochs 20 --lr 0.0002452885525442 --batch_size 64 --wd 0.0003298415408496 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0002452885525442, lr_anneal=15, momentum=0.9, wd=0.0003298415408496, split_index=6, run=10, batch_size=64, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4936\tVal AUC: 0.5510\n",
      "Epoch: [1/20]\tLoss: 0.327831\tTrain AUC: 0.8417\tVal AUC: 0.7657\n",
      "Epoch: [2/20]\tLoss: 0.257642\tTrain AUC: 0.8984\tVal AUC: 0.7793\n",
      "Epoch: [3/20]\tLoss: 0.204463\tTrain AUC: 0.9196\tVal AUC: 0.7600\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.194879\tTrain AUC: 0.9593\tVal AUC: 0.7848\n",
      "Epoch: [5/20]\tLoss: 0.155778\tTrain AUC: 0.9571\tVal AUC: 0.7805\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.125624\tTrain AUC: 0.9800\tVal AUC: 0.7925\n",
      "Epoch: [7/20]\tLoss: 0.102447\tTrain AUC: 0.9786\tVal AUC: 0.7939\n",
      "Epoch: [8/20]\tLoss: 0.085262\tTrain AUC: 0.9962\tVal AUC: 0.7958\n",
      "Epoch: [9/20]\tLoss: 0.072844\tTrain AUC: 0.9833\tVal AUC: 0.7778\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [10/20]\tLoss: 0.053745\tTrain AUC: 0.9900\tVal AUC: 0.8109\n",
      "Epoch: [11/20]\tLoss: 0.083295\tTrain AUC: 0.9772\tVal AUC: 0.7569\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.032760\tTrain AUC: 0.9995\tVal AUC: 0.8232\n",
      "Epoch: [13/20]\tLoss: 0.026887\tTrain AUC: 0.9973\tVal AUC: 0.7708\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 10 --nepochs 20 --lr 0.000347186906366 --batch_size 128 --wd 0.0012005952408967 --optimizer adam --cls_arch complex --dropout 0.1301019009181079 --hidden_dim 128 --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=128, dropout=0.1301019009181079, optimizer='adam', lr=0.000347186906366, lr_anneal=15, momentum=0.9, wd=0.0012005952408967, split_index=7, run=10, batch_size=128, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5033\tVal AUC: 0.4949\n",
      "Epoch: [1/20]\tLoss: 0.327025\tTrain AUC: 0.7841\tVal AUC: 0.6493\n",
      "Epoch: [2/20]\tLoss: 0.238769\tTrain AUC: 0.8935\tVal AUC: 0.7568\n",
      "Epoch: [3/20]\tLoss: 0.201800\tTrain AUC: 0.8886\tVal AUC: 0.6926\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.163188\tTrain AUC: 0.9463\tVal AUC: 0.7993\n",
      "Epoch: [5/20]\tLoss: 0.136893\tTrain AUC: 0.9441\tVal AUC: 0.7328\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.108050\tTrain AUC: 0.9876\tVal AUC: 0.7741\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.063131\tTrain AUC: 0.9969\tVal AUC: 0.7638\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.046894\tTrain AUC: 0.9991\tVal AUC: 0.7795\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 10 --nepochs 20 --lr 0.0002798076914403 --batch_size 16 --wd 0.0003082771512274 --optimizer adam --cls_arch complex --dropout 0.1504799058962193 --hidden_dim 384 --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=384, dropout=0.1504799058962193, optimizer='adam', lr=0.0002798076914403, lr_anneal=15, momentum=0.9, wd=0.0003082771512274, split_index=8, run=10, batch_size=16, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4999\tVal AUC: 0.5123\n",
      "Epoch: [1/20]\tLoss: 0.317192\tTrain AUC: 0.7284\tVal AUC: 0.6157\n",
      "Epoch: [2/20]\tLoss: 0.281781\tTrain AUC: 0.7984\tVal AUC: 0.6841\n",
      "Epoch: [3/20]\tLoss: 0.268158\tTrain AUC: 0.8351\tVal AUC: 0.6970\n",
      "Epoch: [4/20]\tLoss: 0.250808\tTrain AUC: 0.8569\tVal AUC: 0.7453\n",
      "Epoch: [5/20]\tLoss: 0.236830\tTrain AUC: 0.7299\tVal AUC: 0.6613\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.228957\tTrain AUC: 0.9343\tVal AUC: 0.7791\n",
      "Epoch: [7/20]\tLoss: 0.218600\tTrain AUC: 0.9205\tVal AUC: 0.7481\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [8/20]\tLoss: 0.180942\tTrain AUC: 0.9468\tVal AUC: 0.7287\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.132589\tTrain AUC: 0.9898\tVal AUC: 0.7609\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.098112\tTrain AUC: 0.9940\tVal AUC: 0.7637\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 10 --nepochs 20 --lr 0.0054920816430505 --batch_size 64 --wd 0.0091671397758696 --optimizer sgd --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='sgd', lr=0.0054920816430505, lr_anneal=15, momentum=0.9, wd=0.0091671397758696, split_index=9, run=10, batch_size=64, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4520\tVal AUC: 0.4765\n",
      "Epoch: [1/20]\tLoss: 0.350593\tTrain AUC: 0.7607\tVal AUC: 0.7375\n",
      "Epoch: [2/20]\tLoss: 0.301991\tTrain AUC: 0.8209\tVal AUC: 0.7608\n",
      "Epoch: [3/20]\tLoss: 0.271187\tTrain AUC: 0.8600\tVal AUC: 0.7761\n",
      "Epoch: [4/20]\tLoss: 0.215191\tTrain AUC: 0.8710\tVal AUC: 0.7892\n",
      "Epoch: [5/20]\tLoss: 0.233745\tTrain AUC: 0.9165\tVal AUC: 0.8050\n",
      "Epoch: [6/20]\tLoss: 0.195567\tTrain AUC: 0.9627\tVal AUC: 0.7830\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [7/20]\tLoss: 0.154519\tTrain AUC: 0.9831\tVal AUC: 0.8442\n",
      "Epoch: [8/20]\tLoss: 0.108299\tTrain AUC: 0.9604\tVal AUC: 0.8197\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [9/20]\tLoss: 0.104070\tTrain AUC: 0.9867\tVal AUC: 0.8152\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [10/20]\tLoss: 0.102874\tTrain AUC: 0.9848\tVal AUC: 0.8339\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [11/20]\tLoss: 0.062680\tTrain AUC: 0.9905\tVal AUC: 0.7944\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 10 --nepochs 20 --lr 0.0005022449734125 --batch_size 128 --wd 0.0031908005561861 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0005022449734125, lr_anneal=15, momentum=0.9, wd=0.0031908005561861, split_index=10, run=10, batch_size=128, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1970, val:508\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5106\tVal AUC: 0.5127\n",
      "Epoch: [1/20]\tLoss: 0.304959\tTrain AUC: 0.8109\tVal AUC: 0.7165\n",
      "Epoch: [2/20]\tLoss: 0.246445\tTrain AUC: 0.8471\tVal AUC: 0.7700\n",
      "Epoch: [3/20]\tLoss: 0.226433\tTrain AUC: 0.8814\tVal AUC: 0.8043\n",
      "Epoch: [4/20]\tLoss: 0.197797\tTrain AUC: 0.9263\tVal AUC: 0.7710\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.169175\tTrain AUC: 0.8883\tVal AUC: 0.7648\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.173584\tTrain AUC: 0.9209\tVal AUC: 0.7290\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.168862\tTrain AUC: 0.9551\tVal AUC: 0.7728\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.137891\tTrain AUC: 0.9566\tVal AUC: 0.8035\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 11 --run 10 --nepochs 20 --lr 0.0005920046960916 --batch_size 16 --wd 0.0090970230586866 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0005920046960916, lr_anneal=15, momentum=0.9, wd=0.0090970230586866, split_index=11, run=10, batch_size=16, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1960, val:518\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4590\tVal AUC: 0.4910\n",
      "Epoch: [1/20]\tLoss: 0.343284\tTrain AUC: 0.4131\tVal AUC: 0.4402\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [2/20]\tLoss: 0.349879\tTrain AUC: 0.6250\tVal AUC: 0.5752\n",
      "Epoch: [3/20]\tLoss: 0.349650\tTrain AUC: 0.5786\tVal AUC: 0.6031\n",
      "Epoch: [4/20]\tLoss: 0.351751\tTrain AUC: 0.5169\tVal AUC: 0.5026\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.351154\tTrain AUC: 0.4574\tVal AUC: 0.4779\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.341000\tTrain AUC: 0.6721\tVal AUC: 0.6621\n",
      "Epoch: [7/20]\tLoss: 0.330166\tTrain AUC: 0.7097\tVal AUC: 0.6324\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.320770\tTrain AUC: 0.7310\tVal AUC: 0.6673\n",
      "Epoch: [9/20]\tLoss: 0.310151\tTrain AUC: 0.7233\tVal AUC: 0.5889\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 12 --run 10 --nepochs 20 --lr 0.0018325450716217 --batch_size 32 --wd 0.0080493558068109 --optimizer sgd --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='sgd', lr=0.0018325450716217, lr_anneal=15, momentum=0.9, wd=0.0080493558068109, split_index=12, run=10, batch_size=32, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 304, in main\n",
      "    val_loss = validate_loss(val_loader, model, criterion)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 94, in validate_loss\n",
      "    running_loss += loss.item() * input.size(0)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 13 --run 10 --nepochs 20 --lr 0.0020462138489769 --batch_size 32 --wd 0.0011857387115643 --optimizer sgd --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='sgd', lr=0.0020462138489769, lr_anneal=15, momentum=0.9, wd=0.0011857387115643, split_index=13, run=10, batch_size=32, nepochs=20, workers=4, augm=4, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1944, val:534\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4619\tVal AUC: 0.5522\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters for training from scratch\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary_scratch.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row[\"split\"])\n",
    "    \n",
    "    # Retrieve hyperparameters from CSV\n",
    "    lr = row[\"params_lr\"]\n",
    "    batch_size = row[\"params_batch_size\"]\n",
    "    wd = row[\"params_wd\"]\n",
    "    optimizer = row[\"params_optimizer\"]\n",
    "    cls_arch = row[\"params_cls_arch\"]  # e.g., 'simple' or 'complex'\n",
    "    \n",
    "    # If 'params_dropout' or 'params_hidden_dim' are missing or NaN, handle gracefully\n",
    "    dropout_flag = \"\"\n",
    "    if \"params_dropout\" in row and not pd.isna(row[\"params_dropout\"]):\n",
    "        dropout_flag = f\"--dropout {row['params_dropout']}\"\n",
    "    \n",
    "    hidden_dim_flag = \"\"\n",
    "    if \"params_hidden_dim\" in row and not pd.isna(row[\"params_hidden_dim\"]):\n",
    "        # If hidden_dim is a float like 448.0, convert to int if appropriate\n",
    "        hidden_dim_val = int(row[\"params_hidden_dim\"]) if not math.isnan(row[\"params_hidden_dim\"]) else None\n",
    "        if hidden_dim_val is not None:\n",
    "            hidden_dim_flag = f\"--hidden_dim {hidden_dim_val}\"\n",
    "    \n",
    "    # Check if balance is True/1\n",
    "    balance_flag = \"\"\n",
    "    if str(row[\"params_balance\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        balance_flag = \"--balance\"\n",
    "\n",
    "    # Check if LR scheduler is True/1\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row[\"params_lr_scheduler\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # Example augmentation flag (adjust as needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Build the command string\n",
    "    command = (\n",
    "        f\"python train.py \"\n",
    "        f\"--split_index {split} \"\n",
    "        f\"--run 10 \"\n",
    "        f\"--nepochs 20 \"\n",
    "        f\"--lr {lr} \"\n",
    "        f\"--batch_size {batch_size} \"\n",
    "        f\"--wd {wd} \"\n",
    "        f\"--optimizer {optimizer} \"\n",
    "        f\"--cls_arch {cls_arch} \"     # Classification layer architecture\n",
    "        f\"{dropout_flag} \"           # e.g., --dropout 0.48\n",
    "        f\"{hidden_dim_flag} \"        # e.g., --hidden_dim 448\n",
    "        f\"{balance_flag} \"           # e.g., --balance\n",
    "        f\"{lr_scheduler_flag} \"      # e.g., --lr_scheduler\n",
    "        f\"--early_stopping \"\n",
    "        f\"{augm_flag}\"\n",
    "    )\n",
    "    \n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
