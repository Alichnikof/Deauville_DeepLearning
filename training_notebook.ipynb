{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to train using all the best parameters for each split (but using different models) 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters per split\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop over each row (each split) and build the command using the tuned parameters\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row['split'])\n",
    "    # Build checkpoint path as before\n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    \n",
    "    # Get hyperparameters from the CSV row.\n",
    "    lr = row['params_lr']              # e.g., 0.0000143417\n",
    "    batch_size = row['params_batch_size']  # e.g., 128\n",
    "    wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "    optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "    # Use the ft_mode field to decide which flag to pass:\n",
    "    ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "    ft_flag = \"\"\n",
    "    if ft_mode == \"finetune\":\n",
    "        ft_flag = \"--finetune\"\n",
    "    elif ft_mode == \"transfer_learning\":\n",
    "        ft_flag = \"--transfer_learning\"\n",
    "    # for full_retrain, no additional flag is needed\n",
    "    \n",
    "    # If lr_scheduler is True, add that flag.\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # If balance flag is True, add that flag.\n",
    "    balance_flag = \"\"\n",
    "    if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "        balance_flag = \"--balance\"\n",
    "    \n",
    "    # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Construct the command string\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 4 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "        f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "        f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar but just the same checkpoint, probably best approach. 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters per split\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop over each row (each split) and build the command using the tuned parameters\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row['split'])\n",
    "    # Build checkpoint path as before\n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth\"\n",
    "    \n",
    "    # Get hyperparameters from the CSV row.\n",
    "    lr = row['params_lr']              # e.g., 0.0000143417\n",
    "    batch_size = row['params_batch_size']  # e.g., 128\n",
    "    wd = row['params_wd']              # e.g., 7.60351e-06\n",
    "    optimizer = row['params_optimizer']    # e.g., adam\n",
    "    \n",
    "    # Use the ft_mode field to decide which flag to pass:\n",
    "    ft_mode = row['params_ft_mode'].strip().lower()  # should be 'finetune', 'full_retrain' or 'transfer_learning'\n",
    "    ft_flag = \"\"\n",
    "    if ft_mode == \"finetune\":\n",
    "        ft_flag = \"--finetune\"\n",
    "    elif ft_mode == \"transfer_learning\":\n",
    "        ft_flag = \"--transfer_learning\"\n",
    "    # for full_retrain, no additional flag is needed\n",
    "    \n",
    "    # If lr_scheduler is True, add that flag.\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row['params_lr_scheduler']).strip().lower() in ['true', '1']:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # If balance flag is True, add that flag.\n",
    "    balance_flag = \"\"\n",
    "    if str(row['params_balance']).strip().lower() in ['true', '1']:\n",
    "        balance_flag = \"--balance\"\n",
    "    \n",
    "    # Here we fix the augmentation flag as 4 (or you can also tune it if needed)\n",
    "    augm_flag = \"--augm 4\"\n",
    "    \n",
    "    # Construct the command string\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 5 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr {lr} --batch_size {batch_size} --wd {wd} \"\n",
    "        f\"--optimizer {optimizer} --cls_arch simple {balance_flag} {lr_scheduler_flag} \"\n",
    "        f\"--early_stopping {ft_flag} {augm_flag}\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different checkpoint modle but same parameter run 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=0, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8319\tVal AUC: 0.8400\n",
      "Epoch: [1/20]\tLoss: 0.795695\tTrain AUC: 0.8582\tVal AUC: 0.8424\n",
      "Epoch: [2/20]\tLoss: 0.477286\tTrain AUC: 0.8692\tVal AUC: 0.8315\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.373951\tTrain AUC: 0.8812\tVal AUC: 0.8279\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.346648\tTrain AUC: 0.8964\tVal AUC: 0.8303\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.328387\tTrain AUC: 0.9066\tVal AUC: 0.8337\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.304013\tTrain AUC: 0.9202\tVal AUC: 0.8415\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split1_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=1, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8337\tVal AUC: 0.8982\n",
      "Epoch: [1/20]\tLoss: 0.719204\tTrain AUC: 0.8563\tVal AUC: 0.8995\n",
      "Epoch: [2/20]\tLoss: 0.456692\tTrain AUC: 0.8495\tVal AUC: 0.8905\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.377079\tTrain AUC: 0.8715\tVal AUC: 0.8918\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.336437\tTrain AUC: 0.8971\tVal AUC: 0.9008\n",
      "Epoch: [5/20]\tLoss: 0.327745\tTrain AUC: 0.9122\tVal AUC: 0.9022\n",
      "Epoch: [6/20]\tLoss: 0.301314\tTrain AUC: 0.9254\tVal AUC: 0.8963\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.287536\tTrain AUC: 0.9360\tVal AUC: 0.9059\n",
      "Epoch: [8/20]\tLoss: 0.265555\tTrain AUC: 0.9477\tVal AUC: 0.9040\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.249257\tTrain AUC: 0.9565\tVal AUC: 0.9056\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split2_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=2, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8450\tVal AUC: 0.8129\n",
      "Epoch: [1/20]\tLoss: 0.748679\tTrain AUC: 0.8560\tVal AUC: 0.8359\n",
      "Epoch: [2/20]\tLoss: 0.432387\tTrain AUC: 0.8550\tVal AUC: 0.8170\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.369103\tTrain AUC: 0.8729\tVal AUC: 0.8110\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.322029\tTrain AUC: 0.8959\tVal AUC: 0.8268\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.314677\tTrain AUC: 0.9078\tVal AUC: 0.8238\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.284953\tTrain AUC: 0.9192\tVal AUC: 0.8297\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split3_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=3, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8302\tVal AUC: 0.8929\n",
      "Epoch: [1/20]\tLoss: 0.705121\tTrain AUC: 0.8588\tVal AUC: 0.8960\n",
      "Epoch: [2/20]\tLoss: 0.448639\tTrain AUC: 0.8661\tVal AUC: 0.8700\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.362379\tTrain AUC: 0.8818\tVal AUC: 0.8723\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.346205\tTrain AUC: 0.8972\tVal AUC: 0.8799\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.327637\tTrain AUC: 0.9099\tVal AUC: 0.8829\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.317779\tTrain AUC: 0.9213\tVal AUC: 0.8763\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split4_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=4, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8335\tVal AUC: 0.8436\n",
      "Epoch: [1/20]\tLoss: 0.755982\tTrain AUC: 0.8600\tVal AUC: 0.8476\n",
      "Epoch: [2/20]\tLoss: 0.497949\tTrain AUC: 0.8679\tVal AUC: 0.8172\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.371583\tTrain AUC: 0.8849\tVal AUC: 0.8184\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.335742\tTrain AUC: 0.8999\tVal AUC: 0.8248\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.307319\tTrain AUC: 0.9138\tVal AUC: 0.8274\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.300011\tTrain AUC: 0.9233\tVal AUC: 0.8330\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split5_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=5, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8515\tVal AUC: 0.8151\n",
      "Epoch: [1/20]\tLoss: 0.748418\tTrain AUC: 0.8714\tVal AUC: 0.8247\n",
      "Epoch: [2/20]\tLoss: 0.462196\tTrain AUC: 0.8738\tVal AUC: 0.8149\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.376638\tTrain AUC: 0.8882\tVal AUC: 0.8147\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.332610\tTrain AUC: 0.9069\tVal AUC: 0.8241\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.318271\tTrain AUC: 0.9207\tVal AUC: 0.8209\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.287086\tTrain AUC: 0.9343\tVal AUC: 0.8212\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split6_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=6, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8451\tVal AUC: 0.8293\n",
      "Epoch: [1/20]\tLoss: 0.729225\tTrain AUC: 0.8535\tVal AUC: 0.8456\n",
      "Epoch: [2/20]\tLoss: 0.439046\tTrain AUC: 0.8571\tVal AUC: 0.8328\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.376330\tTrain AUC: 0.8778\tVal AUC: 0.8311\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.342079\tTrain AUC: 0.8976\tVal AUC: 0.8360\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.322452\tTrain AUC: 0.9115\tVal AUC: 0.8347\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.307880\tTrain AUC: 0.9251\tVal AUC: 0.8315\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split7_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=7, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8471\tVal AUC: 0.7402\n",
      "Epoch: [1/20]\tLoss: 0.768954\tTrain AUC: 0.8692\tVal AUC: 0.7733\n",
      "Epoch: [2/20]\tLoss: 0.432007\tTrain AUC: 0.8796\tVal AUC: 0.7721\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.356598\tTrain AUC: 0.8907\tVal AUC: 0.7757\n",
      "Epoch: [4/20]\tLoss: 0.313837\tTrain AUC: 0.9025\tVal AUC: 0.7883\n",
      "Epoch: [5/20]\tLoss: 0.310866\tTrain AUC: 0.9132\tVal AUC: 0.7963\n",
      "Epoch: [6/20]\tLoss: 0.291899\tTrain AUC: 0.9213\tVal AUC: 0.8086\n",
      "Epoch: [7/20]\tLoss: 0.271407\tTrain AUC: 0.9295\tVal AUC: 0.8153\n",
      "Epoch: [8/20]\tLoss: 0.265773\tTrain AUC: 0.9407\tVal AUC: 0.8127\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [9/20]\tLoss: 0.244595\tTrain AUC: 0.9447\tVal AUC: 0.8241\n",
      "Epoch: [10/20]\tLoss: 0.243048\tTrain AUC: 0.9526\tVal AUC: 0.8201\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.234959\tTrain AUC: 0.9631\tVal AUC: 0.8158\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.204501\tTrain AUC: 0.9687\tVal AUC: 0.8134\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split8_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=8, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8398\tVal AUC: 0.8253\n",
      "Epoch: [1/20]\tLoss: 0.678181\tTrain AUC: 0.8626\tVal AUC: 0.8333\n",
      "Epoch: [2/20]\tLoss: 0.416904\tTrain AUC: 0.8693\tVal AUC: 0.8234\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.366375\tTrain AUC: 0.8897\tVal AUC: 0.8272\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.316840\tTrain AUC: 0.9099\tVal AUC: 0.8272\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.299458\tTrain AUC: 0.9218\tVal AUC: 0.8348\n",
      "Epoch: [6/20]\tLoss: 0.289612\tTrain AUC: 0.9331\tVal AUC: 0.8381\n",
      "Epoch: [7/20]\tLoss: 0.258733\tTrain AUC: 0.9443\tVal AUC: 0.8383\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.252017\tTrain AUC: 0.9544\tVal AUC: 0.8370\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split9_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=9, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8014\tVal AUC: 0.9042\n",
      "Epoch: [1/20]\tLoss: 0.783342\tTrain AUC: 0.8366\tVal AUC: 0.9206\n",
      "Epoch: [2/20]\tLoss: 0.507437\tTrain AUC: 0.8455\tVal AUC: 0.9169\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.397385\tTrain AUC: 0.8564\tVal AUC: 0.9064\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.360524\tTrain AUC: 0.8755\tVal AUC: 0.9083\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.339109\tTrain AUC: 0.8934\tVal AUC: 0.9191\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.322820\tTrain AUC: 0.9072\tVal AUC: 0.9214\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=10, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split10_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 11 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=11, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split11_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 12 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=12, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split12_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 13 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=13, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split13_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 14 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=14, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split14_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 15 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=15, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split15_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 16 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=16, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split16_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 17 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=17, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split17_run0.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 18 --run 12 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=18, run=12, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/train.py\", line 121, in main\n",
      "    ch = torch.load(args.checkpoint, weights_only=False)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 1425, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 751, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/serialization.py\", line 732, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split18_run0.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# FINE TUNED MODEL\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "for split in range(0, 10): \n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split{split}_run0.pth\"\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 12 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 20 - One checkpoint model loded and we use parameter of the best split AUC - basically what Haggstrom does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 20 --nepochs 20 --checkpoint /home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth --lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 --optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\n",
      "Namespace(output='training_results', normalize=True, checkpoint='/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=1.43417e-05, lr_anneal=15, momentum=0.9, wd=7.60351e-06, split_index=0, run=20, batch_size=128, nepochs=20, workers=4, augm=4, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=True, transfer_learning=False)\n",
      "Loaded [218/218] keys from checkpoint\n",
      "Fine-tuning mode enabled: classifier head and last block of feature extractor are trainable.\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.8319\tVal AUC: 0.8400\n",
      "Epoch: [1/20]\tLoss: 0.783166\tTrain AUC: 0.8583\tVal AUC: 0.8419\n",
      "Epoch: [2/20]\tLoss: 0.465037\tTrain AUC: 0.8676\tVal AUC: 0.8256\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.401630\tTrain AUC: 0.8774\tVal AUC: 0.8213\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.353409\tTrain AUC: 0.8942\tVal AUC: 0.8282\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [5/20]\tLoss: 0.332947\tTrain AUC: 0.9083\tVal AUC: 0.8307\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [6/20]\tLoss: 0.303861\tTrain AUC: 0.9214\tVal AUC: 0.8408\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Best parameter from optuna fine tuning\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "for split in range(0,20): \n",
    "    checkpoint_path = f\"/home/mezher/Documents/Deauville_DeepLearning/checkpoints/checkpoint_split0_run0.pth\"\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 20 --nepochs 20 \"\n",
    "        f\"--checkpoint {checkpoint_path} \"\n",
    "        f\"--lr 0.0000143417 --batch_size 128 --wd 7.60351e-06 \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --finetune --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch - run 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=0, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5194\tVal AUC: 0.5522\n",
      "Epoch: [1/20]\tLoss: 0.310737\tTrain AUC: 0.8226\tVal AUC: 0.7896\n",
      "Epoch: [2/20]\tLoss: 0.281113\tTrain AUC: 0.8334\tVal AUC: 0.7589\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.263155\tTrain AUC: 0.8350\tVal AUC: 0.7502\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.245361\tTrain AUC: 0.8883\tVal AUC: 0.8100\n",
      "Epoch: [5/20]\tLoss: 0.221566\tTrain AUC: 0.9125\tVal AUC: 0.8208\n",
      "Epoch: [6/20]\tLoss: 0.196390\tTrain AUC: 0.9323\tVal AUC: 0.8259\n",
      "Epoch: [7/20]\tLoss: 0.190003\tTrain AUC: 0.9450\tVal AUC: 0.8118\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.168689\tTrain AUC: 0.9544\tVal AUC: 0.7455\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.142522\tTrain AUC: 0.9756\tVal AUC: 0.8059\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=1, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5455\tVal AUC: 0.5505\n",
      "Epoch: [1/20]\tLoss: 0.354043\tTrain AUC: 0.7275\tVal AUC: 0.7314\n",
      "Epoch: [2/20]\tLoss: 0.302688\tTrain AUC: 0.8120\tVal AUC: 0.8358\n",
      "Epoch: [3/20]\tLoss: 0.285615\tTrain AUC: 0.8129\tVal AUC: 0.8015\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.279555\tTrain AUC: 0.7780\tVal AUC: 0.8094\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.250300\tTrain AUC: 0.8832\tVal AUC: 0.8244\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.222595\tTrain AUC: 0.9101\tVal AUC: 0.8453\n",
      "Epoch: [7/20]\tLoss: 0.209828\tTrain AUC: 0.9237\tVal AUC: 0.8448\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.190455\tTrain AUC: 0.9422\tVal AUC: 0.8472\n",
      "Epoch: [9/20]\tLoss: 0.174718\tTrain AUC: 0.9422\tVal AUC: 0.8545\n",
      "Epoch: [10/20]\tLoss: 0.156075\tTrain AUC: 0.9683\tVal AUC: 0.8305\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=2, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5151\tVal AUC: 0.4683\n",
      "Epoch: [1/20]\tLoss: 0.324013\tTrain AUC: 0.7620\tVal AUC: 0.7036\n",
      "Epoch: [2/20]\tLoss: 0.285554\tTrain AUC: 0.7861\tVal AUC: 0.7313\n",
      "Epoch: [3/20]\tLoss: 0.261537\tTrain AUC: 0.8092\tVal AUC: 0.7058\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.256289\tTrain AUC: 0.8579\tVal AUC: 0.7485\n",
      "Epoch: [5/20]\tLoss: 0.250616\tTrain AUC: 0.7351\tVal AUC: 0.6296\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.245091\tTrain AUC: 0.9043\tVal AUC: 0.7562\n",
      "Epoch: [7/20]\tLoss: 0.236785\tTrain AUC: 0.8597\tVal AUC: 0.7706\n",
      "Epoch: [8/20]\tLoss: 0.227779\tTrain AUC: 0.9038\tVal AUC: 0.7632\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.220599\tTrain AUC: 0.8852\tVal AUC: 0.6898\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.205041\tTrain AUC: 0.9254\tVal AUC: 0.7627\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=3, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4669\tVal AUC: 0.4504\n",
      "Epoch: [1/20]\tLoss: 0.349229\tTrain AUC: 0.7250\tVal AUC: 0.7293\n",
      "Epoch: [2/20]\tLoss: 0.305924\tTrain AUC: 0.6908\tVal AUC: 0.6939\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.292436\tTrain AUC: 0.8185\tVal AUC: 0.7868\n",
      "Epoch: [4/20]\tLoss: 0.277017\tTrain AUC: 0.8466\tVal AUC: 0.8126\n",
      "Epoch: [5/20]\tLoss: 0.267696\tTrain AUC: 0.8234\tVal AUC: 0.7402\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.267682\tTrain AUC: 0.8967\tVal AUC: 0.8373\n",
      "Epoch: [7/20]\tLoss: 0.252140\tTrain AUC: 0.8982\tVal AUC: 0.8218\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.231554\tTrain AUC: 0.8984\tVal AUC: 0.7983\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.194689\tTrain AUC: 0.9522\tVal AUC: 0.8467\n",
      "Epoch: [10/20]\tLoss: 0.165737\tTrain AUC: 0.9665\tVal AUC: 0.8285\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=4, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5264\tVal AUC: 0.5544\n",
      "Epoch: [1/20]\tLoss: 0.336440\tTrain AUC: 0.7683\tVal AUC: 0.7241\n",
      "Epoch: [2/20]\tLoss: 0.290280\tTrain AUC: 0.8162\tVal AUC: 0.7673\n",
      "Epoch: [3/20]\tLoss: 0.267709\tTrain AUC: 0.8383\tVal AUC: 0.7341\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.254261\tTrain AUC: 0.7808\tVal AUC: 0.7112\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.217675\tTrain AUC: 0.9217\tVal AUC: 0.8163\n",
      "Epoch: [6/20]\tLoss: 0.179311\tTrain AUC: 0.9454\tVal AUC: 0.8090\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.163487\tTrain AUC: 0.9534\tVal AUC: 0.7933\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.138140\tTrain AUC: 0.9668\tVal AUC: 0.8128\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=5, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5059\tVal AUC: 0.4480\n",
      "Epoch: [1/20]\tLoss: 0.342184\tTrain AUC: 0.7869\tVal AUC: 0.6743\n",
      "Epoch: [2/20]\tLoss: 0.304024\tTrain AUC: 0.8013\tVal AUC: 0.7019\n",
      "Epoch: [3/20]\tLoss: 0.287360\tTrain AUC: 0.8480\tVal AUC: 0.7113\n",
      "Epoch: [4/20]\tLoss: 0.272731\tTrain AUC: 0.7379\tVal AUC: 0.6192\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.258577\tTrain AUC: 0.8750\tVal AUC: 0.7201\n",
      "Epoch: [6/20]\tLoss: 0.254755\tTrain AUC: 0.8636\tVal AUC: 0.7018\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.239783\tTrain AUC: 0.8958\tVal AUC: 0.7605\n",
      "Epoch: [8/20]\tLoss: 0.237737\tTrain AUC: 0.8750\tVal AUC: 0.7318\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.232862\tTrain AUC: 0.9246\tVal AUC: 0.7339\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.176378\tTrain AUC: 0.9532\tVal AUC: 0.7580\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=6, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5136\tVal AUC: 0.5062\n",
      "Epoch: [1/20]\tLoss: 0.345485\tTrain AUC: 0.7697\tVal AUC: 0.7074\n",
      "Epoch: [2/20]\tLoss: 0.295355\tTrain AUC: 0.7885\tVal AUC: 0.7327\n",
      "Epoch: [3/20]\tLoss: 0.297472\tTrain AUC: 0.7729\tVal AUC: 0.7755\n",
      "Epoch: [4/20]\tLoss: 0.287097\tTrain AUC: 0.7186\tVal AUC: 0.6423\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.277186\tTrain AUC: 0.8284\tVal AUC: 0.7845\n",
      "Epoch: [6/20]\tLoss: 0.267600\tTrain AUC: 0.8723\tVal AUC: 0.8178\n",
      "Epoch: [7/20]\tLoss: 0.257377\tTrain AUC: 0.8807\tVal AUC: 0.7828\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [8/20]\tLoss: 0.252358\tTrain AUC: 0.8856\tVal AUC: 0.7761\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.204577\tTrain AUC: 0.9363\tVal AUC: 0.8043\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.189224\tTrain AUC: 0.9503\tVal AUC: 0.8112\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=7, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4999\tVal AUC: 0.4680\n",
      "Epoch: [1/20]\tLoss: 0.332906\tTrain AUC: 0.5337\tVal AUC: 0.4982\n",
      "Epoch: [2/20]\tLoss: 0.292223\tTrain AUC: 0.8083\tVal AUC: 0.7349\n",
      "Epoch: [3/20]\tLoss: 0.273003\tTrain AUC: 0.5490\tVal AUC: 0.5552\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.285639\tTrain AUC: 0.8003\tVal AUC: 0.7667\n",
      "Epoch: [5/20]\tLoss: 0.261858\tTrain AUC: 0.8079\tVal AUC: 0.7335\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.263458\tTrain AUC: 0.8622\tVal AUC: 0.7594\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.220849\tTrain AUC: 0.9174\tVal AUC: 0.7851\n",
      "Epoch: [8/20]\tLoss: 0.187332\tTrain AUC: 0.9398\tVal AUC: 0.7914\n",
      "Epoch: [9/20]\tLoss: 0.168570\tTrain AUC: 0.9542\tVal AUC: 0.8001\n",
      "Epoch: [10/20]\tLoss: 0.159462\tTrain AUC: 0.9554\tVal AUC: 0.7814\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [11/20]\tLoss: 0.147177\tTrain AUC: 0.9690\tVal AUC: 0.7751\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=8, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5318\tVal AUC: 0.5201\n",
      "Epoch: [1/20]\tLoss: 0.351361\tTrain AUC: 0.6945\tVal AUC: 0.5794\n",
      "Epoch: [2/20]\tLoss: 0.304977\tTrain AUC: 0.7913\tVal AUC: 0.6798\n",
      "Epoch: [3/20]\tLoss: 0.292672\tTrain AUC: 0.7623\tVal AUC: 0.6781\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.283116\tTrain AUC: 0.8488\tVal AUC: 0.6812\n",
      "Epoch: [5/20]\tLoss: 0.259144\tTrain AUC: 0.5019\tVal AUC: 0.5090\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.262804\tTrain AUC: 0.8529\tVal AUC: 0.7238\n",
      "Epoch: [7/20]\tLoss: 0.256673\tTrain AUC: 0.8518\tVal AUC: 0.6870\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.248979\tTrain AUC: 0.8520\tVal AUC: 0.7346\n",
      "Epoch: [9/20]\tLoss: 0.244213\tTrain AUC: 0.8620\tVal AUC: 0.7335\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.227244\tTrain AUC: 0.8966\tVal AUC: 0.7551\n",
      "Epoch: [11/20]\tLoss: 0.228118\tTrain AUC: 0.8542\tVal AUC: 0.7089\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=9, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5123\tVal AUC: 0.4783\n",
      "Epoch: [1/20]\tLoss: 0.327452\tTrain AUC: 0.7135\tVal AUC: 0.7284\n",
      "Epoch: [2/20]\tLoss: 0.317032\tTrain AUC: 0.7460\tVal AUC: 0.7662\n",
      "Epoch: [3/20]\tLoss: 0.294491\tTrain AUC: 0.8264\tVal AUC: 0.7805\n",
      "Epoch: [4/20]\tLoss: 0.284092\tTrain AUC: 0.8087\tVal AUC: 0.7843\n",
      "Epoch: [5/20]\tLoss: 0.274418\tTrain AUC: 0.4068\tVal AUC: 0.5153\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.276720\tTrain AUC: 0.8080\tVal AUC: 0.7938\n",
      "Epoch: [7/20]\tLoss: 0.269652\tTrain AUC: 0.8521\tVal AUC: 0.8077\n",
      "Epoch: [8/20]\tLoss: 0.266629\tTrain AUC: 0.8429\tVal AUC: 0.8018\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [9/20]\tLoss: 0.253893\tTrain AUC: 0.8516\tVal AUC: 0.8327\n",
      "Epoch: [10/20]\tLoss: 0.244502\tTrain AUC: 0.8406\tVal AUC: 0.8190\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.240603\tTrain AUC: 0.7293\tVal AUC: 0.7051\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.219443\tTrain AUC: 0.9128\tVal AUC: 0.8462\n",
      "Epoch: [13/20]\tLoss: 0.190652\tTrain AUC: 0.9392\tVal AUC: 0.8345\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=10, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1970, val:508\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5430\tVal AUC: 0.5224\n",
      "Epoch: [1/20]\tLoss: 0.333632\tTrain AUC: 0.7519\tVal AUC: 0.7326\n",
      "Epoch: [2/20]\tLoss: 0.278833\tTrain AUC: 0.8030\tVal AUC: 0.7682\n",
      "Epoch: [3/20]\tLoss: 0.271635\tTrain AUC: 0.8340\tVal AUC: 0.7882\n",
      "Epoch: [4/20]\tLoss: 0.259647\tTrain AUC: 0.7621\tVal AUC: 0.7554\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.260246\tTrain AUC: 0.8550\tVal AUC: 0.7331\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.218087\tTrain AUC: 0.9008\tVal AUC: 0.7953\n",
      "Epoch: [7/20]\tLoss: 0.206800\tTrain AUC: 0.9196\tVal AUC: 0.7964\n",
      "Epoch: [8/20]\tLoss: 0.180977\tTrain AUC: 0.9365\tVal AUC: 0.7996\n",
      "Epoch: [9/20]\tLoss: 0.168286\tTrain AUC: 0.9527\tVal AUC: 0.8059\n",
      "Epoch: [10/20]\tLoss: 0.161493\tTrain AUC: 0.9553\tVal AUC: 0.7879\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.145383\tTrain AUC: 0.9704\tVal AUC: 0.7991\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.120276\tTrain AUC: 0.9810\tVal AUC: 0.8058\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 11 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=11, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1960, val:518\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4707\tVal AUC: 0.4394\n",
      "Epoch: [1/20]\tLoss: 0.338253\tTrain AUC: 0.5573\tVal AUC: 0.5026\n",
      "Epoch: [2/20]\tLoss: 0.309388\tTrain AUC: 0.7761\tVal AUC: 0.6670\n",
      "Epoch: [3/20]\tLoss: 0.279797\tTrain AUC: 0.7354\tVal AUC: 0.7004\n",
      "Epoch: [4/20]\tLoss: 0.279792\tTrain AUC: 0.8172\tVal AUC: 0.7279\n",
      "Epoch: [5/20]\tLoss: 0.266832\tTrain AUC: 0.7749\tVal AUC: 0.7157\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.271567\tTrain AUC: 0.8106\tVal AUC: 0.7087\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.243339\tTrain AUC: 0.8817\tVal AUC: 0.7385\n",
      "Epoch: [8/20]\tLoss: 0.210794\tTrain AUC: 0.8997\tVal AUC: 0.7550\n",
      "Epoch: [9/20]\tLoss: 0.204233\tTrain AUC: 0.9089\tVal AUC: 0.7676\n",
      "Epoch: [10/20]\tLoss: 0.198435\tTrain AUC: 0.9256\tVal AUC: 0.7606\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [11/20]\tLoss: 0.184629\tTrain AUC: 0.9340\tVal AUC: 0.7644\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [12/20]\tLoss: 0.162533\tTrain AUC: 0.9414\tVal AUC: 0.7673\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 12 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=12, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4803\tVal AUC: 0.4864\n",
      "Epoch: [1/20]\tLoss: 0.326144\tTrain AUC: 0.5936\tVal AUC: 0.5462\n",
      "Epoch: [2/20]\tLoss: 0.285635\tTrain AUC: 0.8059\tVal AUC: 0.7466\n",
      "Epoch: [3/20]\tLoss: 0.272516\tTrain AUC: 0.8446\tVal AUC: 0.7449\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.269566\tTrain AUC: 0.8525\tVal AUC: 0.7453\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.233175\tTrain AUC: 0.9072\tVal AUC: 0.8049\n",
      "Epoch: [6/20]\tLoss: 0.199654\tTrain AUC: 0.9333\tVal AUC: 0.8018\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.188453\tTrain AUC: 0.9447\tVal AUC: 0.7961\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.165937\tTrain AUC: 0.9550\tVal AUC: 0.8041\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 13 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=13, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1944, val:534\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4899\tVal AUC: 0.4229\n",
      "Epoch: [1/20]\tLoss: 0.323368\tTrain AUC: 0.8132\tVal AUC: 0.7324\n",
      "Epoch: [2/20]\tLoss: 0.278322\tTrain AUC: 0.6563\tVal AUC: 0.6169\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.262587\tTrain AUC: 0.8629\tVal AUC: 0.7557\n",
      "Epoch: [4/20]\tLoss: 0.250586\tTrain AUC: 0.7919\tVal AUC: 0.6161\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.244674\tTrain AUC: 0.8555\tVal AUC: 0.6635\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.193607\tTrain AUC: 0.9286\tVal AUC: 0.7648\n",
      "Epoch: [7/20]\tLoss: 0.177254\tTrain AUC: 0.9482\tVal AUC: 0.7646\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.162196\tTrain AUC: 0.9607\tVal AUC: 0.7756\n",
      "Epoch: [9/20]\tLoss: 0.143546\tTrain AUC: 0.9731\tVal AUC: 0.7727\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 14 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=14, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2038, val:440\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4766\tVal AUC: 0.4848\n",
      "Epoch: [1/20]\tLoss: 0.337507\tTrain AUC: 0.6989\tVal AUC: 0.6775\n",
      "Epoch: [2/20]\tLoss: 0.297715\tTrain AUC: 0.8048\tVal AUC: 0.8159\n",
      "Epoch: [3/20]\tLoss: 0.299176\tTrain AUC: 0.8141\tVal AUC: 0.8057\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.273676\tTrain AUC: 0.7998\tVal AUC: 0.7574\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.250886\tTrain AUC: 0.8741\tVal AUC: 0.8204\n",
      "Epoch: [6/20]\tLoss: 0.225787\tTrain AUC: 0.9058\tVal AUC: 0.8219\n",
      "Epoch: [7/20]\tLoss: 0.210838\tTrain AUC: 0.9241\tVal AUC: 0.8053\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.196988\tTrain AUC: 0.9402\tVal AUC: 0.7697\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.174178\tTrain AUC: 0.9504\tVal AUC: 0.7918\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 15 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=15, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4915\tVal AUC: 0.5006\n",
      "Epoch: [1/20]\tLoss: 0.324443\tTrain AUC: 0.8026\tVal AUC: 0.6623\n",
      "Epoch: [2/20]\tLoss: 0.278324\tTrain AUC: 0.7934\tVal AUC: 0.6491\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.266092\tTrain AUC: 0.8372\tVal AUC: 0.6924\n",
      "Epoch: [4/20]\tLoss: 0.256408\tTrain AUC: 0.8474\tVal AUC: 0.6745\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.252786\tTrain AUC: 0.8747\tVal AUC: 0.6806\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.215237\tTrain AUC: 0.9076\tVal AUC: 0.7089\n",
      "Epoch: [7/20]\tLoss: 0.195336\tTrain AUC: 0.9204\tVal AUC: 0.7095\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.186737\tTrain AUC: 0.9397\tVal AUC: 0.7234\n",
      "Epoch: [9/20]\tLoss: 0.174149\tTrain AUC: 0.9521\tVal AUC: 0.7417\n",
      "Epoch: [10/20]\tLoss: 0.151151\tTrain AUC: 0.9616\tVal AUC: 0.7439\n",
      "Epoch: [11/20]\tLoss: 0.134053\tTrain AUC: 0.9748\tVal AUC: 0.7535\n",
      "Epoch: [12/20]\tLoss: 0.112070\tTrain AUC: 0.9851\tVal AUC: 0.7665\n",
      "Epoch: [13/20]\tLoss: 0.111935\tTrain AUC: 0.9834\tVal AUC: 0.7294\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 16 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=16, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1844, val:634\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5059\tVal AUC: 0.4968\n",
      "Epoch: [1/20]\tLoss: 0.346668\tTrain AUC: 0.5996\tVal AUC: 0.5320\n",
      "Epoch: [2/20]\tLoss: 0.304362\tTrain AUC: 0.7917\tVal AUC: 0.7811\n",
      "Epoch: [3/20]\tLoss: 0.294903\tTrain AUC: 0.8179\tVal AUC: 0.7878\n",
      "Epoch: [4/20]\tLoss: 0.274423\tTrain AUC: 0.8625\tVal AUC: 0.7963\n",
      "Epoch: [5/20]\tLoss: 0.275768\tTrain AUC: 0.8064\tVal AUC: 0.7320\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.270611\tTrain AUC: 0.8554\tVal AUC: 0.7773\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.231697\tTrain AUC: 0.9029\tVal AUC: 0.8107\n",
      "Epoch: [8/20]\tLoss: 0.208124\tTrain AUC: 0.9234\tVal AUC: 0.8179\n",
      "Epoch: [9/20]\tLoss: 0.198665\tTrain AUC: 0.9400\tVal AUC: 0.8328\n",
      "Epoch: [10/20]\tLoss: 0.178697\tTrain AUC: 0.9479\tVal AUC: 0.8381\n",
      "Epoch: [11/20]\tLoss: 0.167971\tTrain AUC: 0.9629\tVal AUC: 0.8424\n",
      "Epoch: [12/20]\tLoss: 0.147304\tTrain AUC: 0.9686\tVal AUC: 0.8105\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [13/20]\tLoss: 0.144943\tTrain AUC: 0.9805\tVal AUC: 0.8239\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [14/20]\tLoss: 0.113729\tTrain AUC: 0.9842\tVal AUC: 0.8417\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 17 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=17, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4900\tVal AUC: 0.5088\n",
      "Epoch: [1/20]\tLoss: 0.319725\tTrain AUC: 0.7770\tVal AUC: 0.6496\n",
      "Epoch: [2/20]\tLoss: 0.275623\tTrain AUC: 0.8227\tVal AUC: 0.7022\n",
      "Epoch: [3/20]\tLoss: 0.270368\tTrain AUC: 0.8207\tVal AUC: 0.6916\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.271705\tTrain AUC: 0.8239\tVal AUC: 0.7141\n",
      "Epoch: [5/20]\tLoss: 0.260219\tTrain AUC: 0.8181\tVal AUC: 0.7265\n",
      "Epoch: [6/20]\tLoss: 0.253952\tTrain AUC: 0.8180\tVal AUC: 0.6385\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.238922\tTrain AUC: 0.8781\tVal AUC: 0.6979\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.204930\tTrain AUC: 0.9370\tVal AUC: 0.7626\n",
      "Epoch: [9/20]\tLoss: 0.178942\tTrain AUC: 0.9599\tVal AUC: 0.7724\n",
      "Epoch: [10/20]\tLoss: 0.161829\tTrain AUC: 0.9695\tVal AUC: 0.7756\n",
      "Epoch: [11/20]\tLoss: 0.139617\tTrain AUC: 0.9792\tVal AUC: 0.7774\n",
      "Epoch: [12/20]\tLoss: 0.119993\tTrain AUC: 0.9884\tVal AUC: 0.7691\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [13/20]\tLoss: 0.094351\tTrain AUC: 0.9929\tVal AUC: 0.7797\n",
      "Epoch: [14/20]\tLoss: 0.085341\tTrain AUC: 0.9950\tVal AUC: 0.7557\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 18 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=18, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2030, val:448\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5462\tVal AUC: 0.4960\n",
      "Epoch: [1/20]\tLoss: 0.328114\tTrain AUC: 0.7705\tVal AUC: 0.7389\n",
      "Epoch: [2/20]\tLoss: 0.285672\tTrain AUC: 0.8129\tVal AUC: 0.7237\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.277136\tTrain AUC: 0.8317\tVal AUC: 0.7443\n",
      "Epoch: [4/20]\tLoss: 0.278827\tTrain AUC: 0.7185\tVal AUC: 0.6061\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.264381\tTrain AUC: 0.7743\tVal AUC: 0.6741\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.229603\tTrain AUC: 0.9046\tVal AUC: 0.7795\n",
      "Epoch: [7/20]\tLoss: 0.205273\tTrain AUC: 0.9159\tVal AUC: 0.7946\n",
      "Epoch: [8/20]\tLoss: 0.190911\tTrain AUC: 0.9347\tVal AUC: 0.7603\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.186123\tTrain AUC: 0.9396\tVal AUC: 0.7592\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 19 --run 6 --nepochs 20 --early_stopping --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --balance --lr_scheduler --optimizer adam --cls_arch simple --augm 45\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=19, run=6, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1938, val:540\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4755\tVal AUC: 0.4573\n",
      "Epoch: [1/20]\tLoss: 0.352909\tTrain AUC: 0.6478\tVal AUC: 0.5687\n",
      "Epoch: [2/20]\tLoss: 0.309705\tTrain AUC: 0.7900\tVal AUC: 0.7416\n",
      "Epoch: [3/20]\tLoss: 0.287262\tTrain AUC: 0.7758\tVal AUC: 0.7483\n",
      "Epoch: [4/20]\tLoss: 0.277327\tTrain AUC: 0.8183\tVal AUC: 0.7175\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.280268\tTrain AUC: 0.8479\tVal AUC: 0.6936\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.237197\tTrain AUC: 0.9085\tVal AUC: 0.7656\n",
      "Epoch: [7/20]\tLoss: 0.211923\tTrain AUC: 0.9248\tVal AUC: 0.7574\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.195951\tTrain AUC: 0.9381\tVal AUC: 0.7812\n",
      "Epoch: [9/20]\tLoss: 0.178287\tTrain AUC: 0.9513\tVal AUC: 0.7800\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.179014\tTrain AUC: 0.9607\tVal AUC: 0.7609\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# FROM SCRATCH MODELS - Parameter of the best split\n",
    "\n",
    "# value = 0.87421679\n",
    "# params_augm = 45\n",
    "# params_balance = True\n",
    "# params_batch_size = 32\n",
    "# params_cls_arch = simple\n",
    "# params_lr = 0.0007149374872274\n",
    "# params_lr_scheduler = True          -- AVEC CES PARAMETRE LA EN VERT CA A BIEN DONNE DE LA MERDE BIZARREMENT.\n",
    "# params_optimizer = adam\n",
    "# params_oversample = False\n",
    "# params_wd = 0.0004372932534312\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for split in range(20):\n",
    "    command = (\n",
    "        f\"python train.py --split_index {split} --run 6 --nepochs 20 \"\n",
    "        f\"--lr 0.00104 --batch_size 64 --wd 0.000138 --oversample --lr_scheduler \"\n",
    "        f\"--optimizer adam --cls_arch simple --early_stopping --augm 4\"\n",
    "    )\n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTUALLY WE SHOULD PROBABLY RE RUN OPTUN.PY cause now oversampling and augmentation included\n",
    "\n",
    "need to rety 6 with those parameters : \n",
    "\n",
    "The best result was achieved in the run with a value of 0.88161. For that run (number 14, split 3), the optimal training parameters were:\n",
    "\n",
    " Balance: False\n",
    " Batch Size: 16\n",
    " Classifier Architecture: simple\n",
    " Learning Rate: ~0.00444\n",
    " LR Scheduler: Enabled (True)\n",
    " Optimizer: SGD\n",
    " Weight Decay: ~0.00241\n",
    "\n",
    "Since it used a simple classifier architecture, dropout and hidden dimension parameters are not applicable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch best parameter for each split - 10 - Nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python train.py --split_index 0 --run 10 --nepochs 20 --lr 0.0005022148658806 --batch_size 128 --wd 0.006777245416484 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 14 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0005022148658806, lr_anneal=15, momentum=0.9, wd=0.006777245416484, split_index=0, run=10, batch_size=128, nepochs=20, workers=4, augm=14, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5540\tVal AUC: 0.5318\n",
      "Epoch: [1/20]\tLoss: 0.331438\tTrain AUC: 0.7200\tVal AUC: 0.7419\n",
      "Epoch: [2/20]\tLoss: 0.287059\tTrain AUC: 0.7756\tVal AUC: 0.7279\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.276437\tTrain AUC: 0.7656\tVal AUC: 0.7795\n",
      "Epoch: [4/20]\tLoss: 0.264751\tTrain AUC: 0.8064\tVal AUC: 0.8018\n",
      "Epoch: [5/20]\tLoss: 0.253713\tTrain AUC: 0.7984\tVal AUC: 0.7314\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.255796\tTrain AUC: 0.7960\tVal AUC: 0.7312\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.241857\tTrain AUC: 0.8666\tVal AUC: 0.8011\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.231858\tTrain AUC: 0.8730\tVal AUC: 0.8207\n",
      "Epoch: [9/20]\tLoss: 0.213774\tTrain AUC: 0.8907\tVal AUC: 0.8212\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 1 --run 10 --nepochs 20 --lr 0.0007420484987937 --batch_size 32 --wd 0.000312513491152 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 3 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007420484987937, lr_anneal=15, momentum=0.9, wd=0.000312513491152, split_index=1, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=True, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2014, val:464\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5704\tVal AUC: 0.5963\n",
      "Epoch: [1/20]\tLoss: 0.346285\tTrain AUC: 0.5960\tVal AUC: 0.7076\n",
      "Epoch: [2/20]\tLoss: 0.315972\tTrain AUC: 0.7282\tVal AUC: 0.8047\n",
      "Epoch: [3/20]\tLoss: 0.307397\tTrain AUC: 0.6760\tVal AUC: 0.6218\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.297027\tTrain AUC: 0.8036\tVal AUC: 0.8208\n",
      "Epoch: [5/20]\tLoss: 0.278307\tTrain AUC: 0.7822\tVal AUC: 0.7511\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.280652\tTrain AUC: 0.7154\tVal AUC: 0.6344\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.283190\tTrain AUC: 0.6761\tVal AUC: 0.7135\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.279724\tTrain AUC: 0.8323\tVal AUC: 0.7956\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 2 --run 10 --nepochs 20 --lr 0.0002079532413834 --batch_size 16 --wd 0.0004568462804258 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0002079532413834, lr_anneal=15, momentum=0.9, wd=0.0004568462804258, split_index=2, run=10, batch_size=16, nepochs=20, workers=4, augm=34, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1964, val:514\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4516\tVal AUC: 0.4667\n",
      "Epoch: [1/20]\tLoss: 0.306328\tTrain AUC: 0.8357\tVal AUC: 0.7502\n",
      "Epoch: [2/20]\tLoss: 0.268741\tTrain AUC: 0.8533\tVal AUC: 0.7333\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.240506\tTrain AUC: 0.8701\tVal AUC: 0.7665\n",
      "Epoch: [4/20]\tLoss: 0.232920\tTrain AUC: 0.8983\tVal AUC: 0.7614\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.222017\tTrain AUC: 0.8974\tVal AUC: 0.7905\n",
      "Epoch: [6/20]\tLoss: 0.202824\tTrain AUC: 0.9045\tVal AUC: 0.7360\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.213439\tTrain AUC: 0.9218\tVal AUC: 0.7602\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.170181\tTrain AUC: 0.9659\tVal AUC: 0.7766\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 3 --run 10 --nepochs 20 --lr 0.0007149374872274 --batch_size 32 --wd 0.0004372932534312 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 45 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0007149374872274, lr_anneal=15, momentum=0.9, wd=0.0004372932534312, split_index=3, run=10, batch_size=32, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1982, val:496\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5118\tVal AUC: 0.4232\n",
      "Epoch: [1/20]\tLoss: 0.347660\tTrain AUC: 0.7528\tVal AUC: 0.7838\n",
      "Epoch: [2/20]\tLoss: 0.315991\tTrain AUC: 0.6730\tVal AUC: 0.6400\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.301834\tTrain AUC: 0.8140\tVal AUC: 0.7599\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [4/20]\tLoss: 0.266883\tTrain AUC: 0.8683\tVal AUC: 0.8389\n",
      "Epoch: [5/20]\tLoss: 0.242166\tTrain AUC: 0.8917\tVal AUC: 0.8458\n",
      "Epoch: [6/20]\tLoss: 0.240201\tTrain AUC: 0.9160\tVal AUC: 0.8416\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.214015\tTrain AUC: 0.9289\tVal AUC: 0.8336\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.190717\tTrain AUC: 0.9415\tVal AUC: 0.8534\n",
      "Epoch: [9/20]\tLoss: 0.177028\tTrain AUC: 0.9461\tVal AUC: 0.8479\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 4 --run 10 --nepochs 20 --lr 0.0097937719059157 --batch_size 64 --wd 0.0001142742366751 --optimizer sgd --cls_arch complex --dropout 0.4850920327988685 --hidden_dim 512 --balance  --early_stopping --augm 1 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=512, dropout=0.4850920327988685, optimizer='sgd', lr=0.0097937719059157, lr_anneal=15, momentum=0.9, wd=0.0001142742366751, split_index=4, run=10, batch_size=64, nepochs=20, workers=4, augm=1, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1948, val:530\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1528  420]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5423\tVal AUC: 0.5510\n",
      "Epoch: [1/20]\tLoss: 0.448867\tTrain AUC: 0.7230\tVal AUC: 0.6616\n",
      "Epoch: [2/20]\tLoss: 0.387010\tTrain AUC: 0.8072\tVal AUC: 0.7416\n",
      "Epoch: [3/20]\tLoss: 0.341089\tTrain AUC: 0.8348\tVal AUC: 0.7811\n",
      "Epoch: [4/20]\tLoss: 0.325111\tTrain AUC: 0.8180\tVal AUC: 0.7214\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.288714\tTrain AUC: 0.8708\tVal AUC: 0.7495\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.279654\tTrain AUC: 0.9238\tVal AUC: 0.7551\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.258986\tTrain AUC: 0.9200\tVal AUC: 0.7521\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.218097\tTrain AUC: 0.9530\tVal AUC: 0.7773\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 5 --run 10 --nepochs 20 --lr 0.000219971883187 --batch_size 128 --wd 0.0053015787340158 --optimizer adam --cls_arch simple     --early_stopping --augm 3 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.000219971883187, lr_anneal=15, momentum=0.9, wd=0.0053015787340158, split_index=5, run=10, batch_size=128, nepochs=20, workers=4, augm=3, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5122\tVal AUC: 0.4901\n",
      "Epoch: [1/20]\tLoss: 0.457651\tTrain AUC: 0.7988\tVal AUC: 0.6749\n",
      "Epoch: [2/20]\tLoss: 0.337533\tTrain AUC: 0.8555\tVal AUC: 0.7738\n",
      "Epoch: [3/20]\tLoss: 0.299161\tTrain AUC: 0.9311\tVal AUC: 0.7249\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.245586\tTrain AUC: 0.9443\tVal AUC: 0.7557\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.229889\tTrain AUC: 0.9492\tVal AUC: 0.7084\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.214439\tTrain AUC: 0.9790\tVal AUC: 0.7510\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.195860\tTrain AUC: 0.9482\tVal AUC: 0.7052\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 6 --run 10 --nepochs 20 --lr 0.000101887888748 --batch_size 16 --wd 0.0007386112995825 --optimizer adam --cls_arch simple    --lr_scheduler --early_stopping --augm 5 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.000101887888748, lr_anneal=15, momentum=0.9, wd=0.0007386112995825, split_index=6, run=10, batch_size=16, nepochs=20, workers=4, augm=5, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1994, val:484\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5011\tVal AUC: 0.5010\n",
      "Epoch: [1/20]\tLoss: 0.485801\tTrain AUC: 0.8312\tVal AUC: 0.7920\n",
      "Epoch: [2/20]\tLoss: 0.394236\tTrain AUC: 0.8957\tVal AUC: 0.7827\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.338481\tTrain AUC: 0.9309\tVal AUC: 0.8279\n",
      "Epoch: [4/20]\tLoss: 0.307983\tTrain AUC: 0.9550\tVal AUC: 0.7768\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.257499\tTrain AUC: 0.9518\tVal AUC: 0.7812\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.186486\tTrain AUC: 0.9916\tVal AUC: 0.8016\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.138368\tTrain AUC: 0.9962\tVal AUC: 0.7940\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 7 --run 10 --nepochs 20 --lr 0.0002221924576152 --batch_size 16 --wd 0.000296459964069 --optimizer adam --cls_arch complex --dropout 0.1429591994388154 --hidden_dim 512  --lr_scheduler --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=512, dropout=0.1429591994388154, optimizer='adam', lr=0.0002221924576152, lr_anneal=15, momentum=0.9, wd=0.000296459964069, split_index=7, run=10, batch_size=16, nepochs=20, workers=4, augm=34, balance=False, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2006, val:472\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5341\tVal AUC: 0.4865\n",
      "Epoch: [1/20]\tLoss: 0.447890\tTrain AUC: 0.7701\tVal AUC: 0.6539\n",
      "Epoch: [2/20]\tLoss: 0.409936\tTrain AUC: 0.8266\tVal AUC: 0.7238\n",
      "Epoch: [3/20]\tLoss: 0.385474\tTrain AUC: 0.8511\tVal AUC: 0.7903\n",
      "Epoch: [4/20]\tLoss: 0.368492\tTrain AUC: 0.8694\tVal AUC: 0.7838\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.356352\tTrain AUC: 0.8952\tVal AUC: 0.7561\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.300956\tTrain AUC: 0.9300\tVal AUC: 0.7995\n",
      "Epoch: [7/20]\tLoss: 0.276611\tTrain AUC: 0.9465\tVal AUC: 0.7992\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.252930\tTrain AUC: 0.9575\tVal AUC: 0.7930\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.232499\tTrain AUC: 0.9627\tVal AUC: 0.7961\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 8 --run 10 --nepochs 20 --lr 0.0004081745903797 --batch_size 32 --wd 0.000339025460612 --optimizer adam --cls_arch simple   --balance  --early_stopping --augm 3 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0004081745903797, lr_anneal=15, momentum=0.9, wd=0.000339025460612, split_index=8, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1974, val:504\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1564  410]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5085\tVal AUC: 0.5124\n",
      "Epoch: [1/20]\tLoss: 0.373619\tTrain AUC: 0.8232\tVal AUC: 0.7090\n",
      "Epoch: [2/20]\tLoss: 0.307419\tTrain AUC: 0.8350\tVal AUC: 0.6946\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.296843\tTrain AUC: 0.8792\tVal AUC: 0.7436\n",
      "Epoch: [4/20]\tLoss: 0.269791\tTrain AUC: 0.9035\tVal AUC: 0.7448\n",
      "Epoch: [5/20]\tLoss: 0.249515\tTrain AUC: 0.9288\tVal AUC: 0.7186\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.216325\tTrain AUC: 0.9410\tVal AUC: 0.7824\n",
      "Epoch: [7/20]\tLoss: 0.213306\tTrain AUC: 0.9571\tVal AUC: 0.7299\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.166221\tTrain AUC: 0.9663\tVal AUC: 0.7354\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.140904\tTrain AUC: 0.9473\tVal AUC: 0.7007\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 9 --run 10 --nepochs 20 --lr 0.0001005048871963 --batch_size 16 --wd 0.0001003076114388 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 5 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0001005048871963, lr_anneal=15, momentum=0.9, wd=0.0001003076114388, split_index=9, run=10, batch_size=16, nepochs=20, workers=4, augm=5, balance=True, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1992, val:486\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1576  416]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5059\tVal AUC: 0.4869\n",
      "Epoch: [1/20]\tLoss: 0.378890\tTrain AUC: 0.8681\tVal AUC: 0.7766\n",
      "Epoch: [2/20]\tLoss: 0.283432\tTrain AUC: 0.9481\tVal AUC: 0.7874\n",
      "Epoch: [3/20]\tLoss: 0.212305\tTrain AUC: 0.9694\tVal AUC: 0.7579\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.141103\tTrain AUC: 0.9730\tVal AUC: 0.7829\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.116761\tTrain AUC: 0.9946\tVal AUC: 0.8064\n",
      "Epoch: [6/20]\tLoss: 0.087561\tTrain AUC: 0.9968\tVal AUC: 0.7998\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.068126\tTrain AUC: 0.9983\tVal AUC: 0.8020\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.060069\tTrain AUC: 0.9990\tVal AUC: 0.8121\n",
      "Epoch: [9/20]\tLoss: 0.050082\tTrain AUC: 0.9998\tVal AUC: 0.8118\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 10 --run 10 --nepochs 20 --lr 0.0001234129879286 --batch_size 64 --wd 0.0018556624076787 --optimizer adam --cls_arch complex --dropout 0.2338216995325822 --hidden_dim 320 --balance  --early_stopping --augm 4 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=320, dropout=0.2338216995325822, optimizer='adam', lr=0.0001234129879286, lr_anneal=15, momentum=0.9, wd=0.0018556624076787, split_index=10, run=10, batch_size=64, nepochs=20, workers=4, augm=4, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1970, val:508\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1584  386]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5087\tVal AUC: 0.5124\n",
      "Epoch: [1/20]\tLoss: 0.375623\tTrain AUC: 0.8565\tVal AUC: 0.7462\n",
      "Epoch: [2/20]\tLoss: 0.221730\tTrain AUC: 0.9605\tVal AUC: 0.7925\n",
      "Epoch: [3/20]\tLoss: 0.168074\tTrain AUC: 0.9535\tVal AUC: 0.7435\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.111285\tTrain AUC: 0.9910\tVal AUC: 0.8097\n",
      "Epoch: [5/20]\tLoss: 0.065976\tTrain AUC: 0.9894\tVal AUC: 0.7711\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.076369\tTrain AUC: 0.9983\tVal AUC: 0.7690\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.050679\tTrain AUC: 0.9972\tVal AUC: 0.7375\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.042585\tTrain AUC: 0.9993\tVal AUC: 0.7775\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 11 --run 10 --nepochs 20 --lr 0.0002758804430064 --batch_size 32 --wd 0.003536740690297 --optimizer adam --cls_arch complex --dropout 0.4917585043090337 --hidden_dim 512   --early_stopping --augm 3 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=512, dropout=0.4917585043090337, optimizer='adam', lr=0.0002758804430064, lr_anneal=15, momentum=0.9, wd=0.003536740690297, split_index=11, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=False, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1960, val:518\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1554  406]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5356\tVal AUC: 0.4793\n",
      "Epoch: [1/20]\tLoss: 0.574903\tTrain AUC: 0.8594\tVal AUC: 0.6745\n",
      "Epoch: [2/20]\tLoss: 0.497997\tTrain AUC: 0.9039\tVal AUC: 0.7078\n",
      "Epoch: [3/20]\tLoss: 0.443926\tTrain AUC: 0.9159\tVal AUC: 0.6844\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.404164\tTrain AUC: 0.8951\tVal AUC: 0.7377\n",
      "Epoch: [5/20]\tLoss: 0.379706\tTrain AUC: 0.9280\tVal AUC: 0.7205\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.353353\tTrain AUC: 0.9437\tVal AUC: 0.7726\n",
      "Epoch: [7/20]\tLoss: 0.328760\tTrain AUC: 0.8765\tVal AUC: 0.6136\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.274023\tTrain AUC: 0.9595\tVal AUC: 0.7101\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.255533\tTrain AUC: 0.9693\tVal AUC: 0.7193\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 12 --run 10 --nepochs 20 --lr 0.0001140826256962 --batch_size 16 --wd 0.0057522507012122 --optimizer adam --cls_arch complex --dropout 0.3491766526560349 --hidden_dim 256 --balance --lr_scheduler --early_stopping --augm 0 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=256, dropout=0.3491766526560349, optimizer='adam', lr=0.0001140826256962, lr_anneal=15, momentum=0.9, wd=0.0057522507012122, split_index=12, run=10, batch_size=16, nepochs=20, workers=4, augm=0, balance=True, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1582  422]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5213\tVal AUC: 0.5206\n",
      "Epoch: [1/20]\tLoss: 0.361528\tTrain AUC: 0.8771\tVal AUC: 0.7768\n",
      "Epoch: [2/20]\tLoss: 0.292594\tTrain AUC: 0.9262\tVal AUC: 0.7637\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [3/20]\tLoss: 0.240617\tTrain AUC: 0.9304\tVal AUC: 0.7837\n",
      "Epoch: [4/20]\tLoss: 0.216340\tTrain AUC: 0.9706\tVal AUC: 0.7839\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.165108\tTrain AUC: 0.9738\tVal AUC: 0.7552\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.165410\tTrain AUC: 0.9680\tVal AUC: 0.7635\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.105391\tTrain AUC: 0.9978\tVal AUC: 0.7909\n",
      "Epoch: [8/20]\tLoss: 0.056227\tTrain AUC: 0.9991\tVal AUC: 0.7894\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 13 --run 10 --nepochs 20 --lr 0.0001392717763116 --batch_size 16 --wd 0.006459529813935 --optimizer adam --cls_arch simple     --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0001392717763116, lr_anneal=15, momentum=0.9, wd=0.006459529813935, split_index=13, run=10, batch_size=16, nepochs=20, workers=4, augm=34, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1944, val:534\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4700\tVal AUC: 0.4840\n",
      "Epoch: [1/20]\tLoss: 0.433153\tTrain AUC: 0.8446\tVal AUC: 0.7476\n",
      "Epoch: [2/20]\tLoss: 0.389498\tTrain AUC: 0.8467\tVal AUC: 0.7856\n",
      "Epoch: [3/20]\tLoss: 0.378434\tTrain AUC: 0.8937\tVal AUC: 0.8155\n",
      "Epoch: [4/20]\tLoss: 0.348424\tTrain AUC: 0.6961\tVal AUC: 0.5368\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [5/20]\tLoss: 0.330806\tTrain AUC: 0.8977\tVal AUC: 0.7518\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.320535\tTrain AUC: 0.9201\tVal AUC: 0.7776\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.304833\tTrain AUC: 0.9221\tVal AUC: 0.7787\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.304653\tTrain AUC: 0.9330\tVal AUC: 0.7560\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 14 --run 10 --nepochs 20 --lr 0.0003776890138046 --batch_size 64 --wd 0.0005177065386227 --optimizer adam --cls_arch simple     --early_stopping --augm 34 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0003776890138046, lr_anneal=15, momentum=0.9, wd=0.0005177065386227, split_index=14, run=10, batch_size=64, nepochs=20, workers=4, augm=34, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2038, val:440\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5574\tVal AUC: 0.5707\n",
      "Epoch: [1/20]\tLoss: 0.458202\tTrain AUC: 0.7748\tVal AUC: 0.7078\n",
      "Epoch: [2/20]\tLoss: 0.382938\tTrain AUC: 0.8408\tVal AUC: 0.7701\n",
      "Epoch: [3/20]\tLoss: 0.346368\tTrain AUC: 0.8630\tVal AUC: 0.7747\n",
      "Epoch: [4/20]\tLoss: 0.335473\tTrain AUC: 0.9166\tVal AUC: 0.7845\n",
      "Epoch: [5/20]\tLoss: 0.320319\tTrain AUC: 0.9008\tVal AUC: 0.8327\n",
      "Epoch: [6/20]\tLoss: 0.293291\tTrain AUC: 0.9014\tVal AUC: 0.8193\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [7/20]\tLoss: 0.268305\tTrain AUC: 0.9371\tVal AUC: 0.7864\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [8/20]\tLoss: 0.259940\tTrain AUC: 0.9427\tVal AUC: 0.7487\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.228608\tTrain AUC: 0.9322\tVal AUC: 0.7021\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.234013\tTrain AUC: 0.9731\tVal AUC: 0.7755\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 15 --run 10 --nepochs 20 --lr 0.0001089155961482 --batch_size 16 --wd 0.0002620693930125 --optimizer adam --cls_arch simple     --early_stopping --augm 0 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0001089155961482, lr_anneal=15, momentum=0.9, wd=0.0002620693930125, split_index=15, run=10, batch_size=16, nepochs=20, workers=4, augm=0, balance=False, oversample=False, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1976, val:502\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing early stopping\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5037\tVal AUC: 0.5138\n",
      "Epoch: [1/20]\tLoss: 0.446064\tTrain AUC: 0.8526\tVal AUC: 0.6598\n",
      "Epoch: [2/20]\tLoss: 0.354659\tTrain AUC: 0.9084\tVal AUC: 0.7276\n",
      "Epoch: [3/20]\tLoss: 0.291965\tTrain AUC: 0.9537\tVal AUC: 0.7105\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.240440\tTrain AUC: 0.9736\tVal AUC: 0.7341\n",
      "Epoch: [5/20]\tLoss: 0.215081\tTrain AUC: 0.9845\tVal AUC: 0.7390\n",
      "Epoch: [6/20]\tLoss: 0.164610\tTrain AUC: 0.9951\tVal AUC: 0.7233\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.116444\tTrain AUC: 0.9991\tVal AUC: 0.7451\n",
      "Epoch: [8/20]\tLoss: 0.085400\tTrain AUC: 0.9984\tVal AUC: 0.6954\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [9/20]\tLoss: 0.114622\tTrain AUC: 0.9993\tVal AUC: 0.7438\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [10/20]\tLoss: 0.078751\tTrain AUC: 0.9964\tVal AUC: 0.7432\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 16 --run 10 --nepochs 20 --lr 0.0003486957008311 --batch_size 16 --wd 0.0001117330008294 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 45 \n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0003486957008311, lr_anneal=15, momentum=0.9, wd=0.0001117330008294, split_index=16, run=10, batch_size=16, nepochs=20, workers=4, augm=45, balance=True, oversample=False, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1844, val:634\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4471\tVal AUC: 0.3997\n",
      "Epoch: [1/20]\tLoss: 0.329708\tTrain AUC: 0.7010\tVal AUC: 0.6780\n",
      "Epoch: [2/20]\tLoss: 0.298605\tTrain AUC: 0.8249\tVal AUC: 0.7857\n",
      "Epoch: [3/20]\tLoss: 0.279961\tTrain AUC: 0.8983\tVal AUC: 0.7312\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.261463\tTrain AUC: 0.8796\tVal AUC: 0.8052\n",
      "Epoch: [5/20]\tLoss: 0.252217\tTrain AUC: 0.8929\tVal AUC: 0.8004\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.237168\tTrain AUC: 0.8890\tVal AUC: 0.8069\n",
      "Epoch: [7/20]\tLoss: 0.228946\tTrain AUC: 0.9204\tVal AUC: 0.8197\n",
      "Epoch: [8/20]\tLoss: 0.213400\tTrain AUC: 0.9655\tVal AUC: 0.8556\n",
      "Epoch: [9/20]\tLoss: 0.192179\tTrain AUC: 0.9309\tVal AUC: 0.7957\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [10/20]\tLoss: 0.176715\tTrain AUC: 0.9747\tVal AUC: 0.8402\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [11/20]\tLoss: 0.113026\tTrain AUC: 0.9934\tVal AUC: 0.8306\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 17 --run 10 --nepochs 20 --lr 0.0060680420850956 --batch_size 32 --wd 0.0092216887956736 --optimizer sgd --cls_arch simple   --balance  --early_stopping --augm 3 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='sgd', lr=0.0060680420850956, lr_anneal=15, momentum=0.9, wd=0.0092216887956736, split_index=17, run=10, batch_size=32, nepochs=20, workers=4, augm=3, balance=True, oversample=True, lr_scheduler=False, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2004, val:474\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing early stopping\n",
      "Class counts: [1578  426]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5203\tVal AUC: 0.4810\n",
      "Epoch: [1/20]\tLoss: 0.378115\tTrain AUC: 0.8051\tVal AUC: 0.6630\n",
      "Epoch: [2/20]\tLoss: 0.314077\tTrain AUC: 0.8980\tVal AUC: 0.6856\n",
      "Epoch: [3/20]\tLoss: 0.267348\tTrain AUC: 0.8157\tVal AUC: 0.5588\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.230771\tTrain AUC: 0.9278\tVal AUC: 0.7073\n",
      "Epoch: [5/20]\tLoss: 0.193334\tTrain AUC: 0.9431\tVal AUC: 0.6522\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [6/20]\tLoss: 0.186062\tTrain AUC: 0.9436\tVal AUC: 0.6640\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [7/20]\tLoss: 0.157214\tTrain AUC: 0.9695\tVal AUC: 0.7011\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [8/20]\tLoss: 0.135757\tTrain AUC: 0.9682\tVal AUC: 0.7359\n",
      "Epoch: [9/20]\tLoss: 0.146961\tTrain AUC: 0.9840\tVal AUC: 0.7112\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 18 --run 10 --nepochs 20 --lr 0.0001898803157707 --batch_size 128 --wd 0.0044579769301155 --optimizer adam --cls_arch complex --dropout 0.4100296709266997 --hidden_dim 448  --lr_scheduler --early_stopping --augm 12 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='complex', hidden_dim=448, dropout=0.4100296709266997, optimizer='adam', lr=0.0001898803157707, lr_anneal=15, momentum=0.9, wd=0.0044579769301155, split_index=18, run=10, batch_size=128, nepochs=20, workers=4, augm=12, balance=False, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:2030, val:448\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1588  442]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.5024\tVal AUC: 0.4924\n",
      "Epoch: [1/20]\tLoss: 0.644536\tTrain AUC: 0.7452\tVal AUC: 0.6766\n",
      "Epoch: [2/20]\tLoss: 0.535107\tTrain AUC: 0.8129\tVal AUC: 0.7431\n",
      "Epoch: [3/20]\tLoss: 0.504286\tTrain AUC: 0.8518\tVal AUC: 0.7529\n",
      "Epoch: [4/20]\tLoss: 0.495932\tTrain AUC: 0.8652\tVal AUC: 0.8040\n",
      "Epoch: [5/20]\tLoss: 0.468401\tTrain AUC: 0.8668\tVal AUC: 0.6818\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [6/20]\tLoss: 0.440457\tTrain AUC: 0.8851\tVal AUC: 0.6862\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [7/20]\tLoss: 0.424730\tTrain AUC: 0.9064\tVal AUC: 0.7589\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [8/20]\tLoss: 0.381820\tTrain AUC: 0.9146\tVal AUC: 0.7707\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [9/20]\tLoss: 0.357549\tTrain AUC: 0.9231\tVal AUC: 0.7827\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n",
      "Running command: python train.py --split_index 19 --run 10 --nepochs 20 --lr 0.0002774186743079 --batch_size 64 --wd 0.0021944169352349 --optimizer adam --cls_arch simple   --balance --lr_scheduler --early_stopping --augm 34 --oversample\n",
      "Namespace(output='training_results', normalize=True, checkpoint='', resume=False, cls_arch='simple', hidden_dim=256, dropout=0.3, optimizer='adam', lr=0.0002774186743079, lr_anneal=15, momentum=0.9, wd=0.0021944169352349, split_index=19, run=10, batch_size=64, nepochs=20, workers=4, augm=34, balance=True, oversample=True, lr_scheduler=True, early_stopping=True, finetune=False, transfer_learning=False)\n",
      "Datasets train:1938, val:540\n",
      "Weight of each class, no tumor: 0.2032828282828283, tumor: 0.7967171717171717\n",
      "Balance loss with weights: [np.float64(0.2032828282828283), np.float64(0.7967171717171717)]\n",
      "INFO: Initializing learning rate scheduler\n",
      "INFO: Initializing early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezher/Documents/Deauville_DeepLearning/thesis/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1506  432]\n",
      "Using oversampling with WeightedRandomSampler for training.\n",
      "Epoch: [0/20]\tLoss: nan\tTrain AUC: 0.4653\tVal AUC: 0.4546\n",
      "Epoch: [1/20]\tLoss: 0.367613\tTrain AUC: 0.8387\tVal AUC: 0.7273\n",
      "Epoch: [2/20]\tLoss: 0.278436\tTrain AUC: 0.9140\tVal AUC: 0.7843\n",
      "Epoch: [3/20]\tLoss: 0.239948\tTrain AUC: 0.9360\tVal AUC: 0.7508\n",
      "INFO: Early stopping counter 1 of 5\n",
      "Epoch: [4/20]\tLoss: 0.221046\tTrain AUC: 0.9490\tVal AUC: 0.7484\n",
      "INFO: Early stopping counter 2 of 5\n",
      "Epoch: [5/20]\tLoss: 0.169815\tTrain AUC: 0.9820\tVal AUC: 0.7528\n",
      "INFO: Early stopping counter 3 of 5\n",
      "Epoch: [6/20]\tLoss: 0.118043\tTrain AUC: 0.9894\tVal AUC: 0.7674\n",
      "INFO: Early stopping counter 4 of 5\n",
      "Epoch: [7/20]\tLoss: 0.099867\tTrain AUC: 0.9928\tVal AUC: 0.7582\n",
      "INFO: Early stopping counter 5 of 5\n",
      "INFO: Early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Set the CUDA device if desired\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Path to the CSV file with best parameters for training from scratch\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "for _, row in best_params.iterrows():\n",
    "    split = int(row[\"split\"])\n",
    "    \n",
    "    # Retrieve hyperparameters from CSV\n",
    "    lr = row[\"params_lr\"]\n",
    "    batch_size = row[\"params_batch_size\"]\n",
    "    wd = row[\"params_wd\"]\n",
    "    optimizer = row[\"params_optimizer\"]\n",
    "    cls_arch = row[\"params_cls_arch\"]  # e.g., 'simple' or 'complex'\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Dropout and hidden_dim flags\n",
    "    # ----------------------------\n",
    "    dropout_flag = \"\"\n",
    "    if \"params_dropout\" in row and not pd.isna(row[\"params_dropout\"]):\n",
    "        dropout_flag = f\"--dropout {row['params_dropout']}\"\n",
    "    \n",
    "    hidden_dim_flag = \"\"\n",
    "    if \"params_hidden_dim\" in row and not pd.isna(row[\"params_hidden_dim\"]):\n",
    "        hidden_dim_val = row[\"params_hidden_dim\"]\n",
    "        # If hidden_dim is non-integer but numeric, convert to int if appropriate\n",
    "        if not math.isnan(hidden_dim_val):\n",
    "            hidden_dim_flag = f\"--hidden_dim {int(hidden_dim_val)}\"\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Balance and LR scheduler\n",
    "    # ----------------------------\n",
    "    balance_flag = \"\"\n",
    "    if str(row[\"params_balance\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        balance_flag = \"--balance\"\n",
    "\n",
    "    lr_scheduler_flag = \"\"\n",
    "    if str(row[\"params_lr_scheduler\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        lr_scheduler_flag = \"--lr_scheduler\"\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # Parse params_augm for the --augm <val> option\n",
    "    # ---------------------------------------------\n",
    "    augm_flag = \"\"\n",
    "    if \"params_augm\" in row and not pd.isna(row[\"params_augm\"]):\n",
    "        augm_val = int(row[\"params_augm\"])  # ensure it's an integer\n",
    "        augm_flag = f\"--augm {augm_val}\"\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # Parse params_oversample for the --oversample\n",
    "    # ---------------------------------------------\n",
    "    oversample_flag = \"\"\n",
    "    if str(row[\"params_oversample\"]).strip().lower() in [\"true\", \"1\"]:\n",
    "        oversample_flag = \"--oversample\"\n",
    "    \n",
    "    # Build the command string\n",
    "    command = (\n",
    "        f\"python train.py \"\n",
    "        f\"--split_index {split} \"\n",
    "        f\"--run 10 \"\n",
    "        f\"--nepochs 20 \"\n",
    "        f\"--lr {lr} \"\n",
    "        f\"--batch_size {int(batch_size)} \"\n",
    "        f\"--wd {wd} \"\n",
    "        f\"--optimizer {optimizer} \"\n",
    "        f\"--cls_arch {cls_arch} \"      # Classification layer architecture\n",
    "        f\"{dropout_flag} \"            # e.g., --dropout 0.48\n",
    "        f\"{hidden_dim_flag} \"         # e.g., --hidden_dim 448\n",
    "        f\"{balance_flag} \"            # e.g., --balance\n",
    "        f\"{lr_scheduler_flag} \"       # e.g., --lr_scheduler\n",
    "        f\"--early_stopping \"\n",
    "        f\"{augm_flag} \"               # e.g., --augm 3\n",
    "        f\"{oversample_flag}\"          # e.g., --oversample\n",
    "    )\n",
    "    \n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 0 0 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 1 1 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 2 2 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 3 3 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 4 4 --cls_arch complex --hidden_dim 512 --dropout 0.4850920327988685\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 5 5 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 6 6 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 7 7 --cls_arch complex --hidden_dim 512 --dropout 0.1429591994388154\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 8 8 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 9 9 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 10 10 --cls_arch complex --hidden_dim 320 --dropout 0.2338216995325822\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 11 11 --cls_arch complex --hidden_dim 512 --dropout 0.4917585043090337\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 12 12 --cls_arch complex --hidden_dim 256 --dropout 0.3491766526560349\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 13 13 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 14 14 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 15 15 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 16 16 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 17 17 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 18 18 --cls_arch complex --hidden_dim 448 --dropout 0.4100296709266997\n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n",
      "Running command: python predict.py --output /home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10 --chptfolder /home/mezher/Documents/Deauville_DeepLearning/training_results --normalize --splits 19 19 --cls_arch simple  \n",
      "  dataset size: 690\n",
      "  loop time: 0 min\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Path to the CSV file with best parameters for inference\n",
    "csv_path = \"/home/mezher/Documents/Deauville_DeepLearning/best_params_summary.csv\"\n",
    "best_params = pd.read_csv(csv_path)\n",
    "\n",
    "# For example, we predict splits 0..19 in one loop:\n",
    "for split_index in range(20):\n",
    "    \n",
    "    # 1) Get the row of best_params for the given split\n",
    "    row = best_params[best_params[\"split\"] == split_index]\n",
    "    if len(row) == 0:\n",
    "        print(f\"No row found in best_params for split={split_index}. Skipping.\")\n",
    "        continue\n",
    "    row = row.iloc[0]  # take the first matching row if multiple\n",
    "    \n",
    "    # 2) Basic arguments\n",
    "    chptfolder = \"/home/mezher/Documents/Deauville_DeepLearning/training_results\"\n",
    "    output_dir = \"/home/mezher/Documents/Deauville_DeepLearning/prediction/scratch/Run10\"\n",
    "    \n",
    "    # 3) Determine which architecture: simple or complex\n",
    "    cls_arch = str(row[\"params_cls_arch\"]).strip()\n",
    "    arch_flag = f\"--cls_arch {cls_arch}\"\n",
    "    \n",
    "    # 4) If complex, parse hidden_dim and dropout\n",
    "    hidden_dim_flag = \"\"\n",
    "    dropout_flag = \"\"\n",
    "    if cls_arch == \"complex\":\n",
    "        # Hidden dim (if present)\n",
    "        if \"params_hidden_dim\" in row and not pd.isna(row[\"params_hidden_dim\"]):\n",
    "            hd_val = int(row[\"params_hidden_dim\"])\n",
    "            hidden_dim_flag = f\"--hidden_dim {hd_val}\"\n",
    "        # Dropout\n",
    "        if \"params_dropout\" in row and not pd.isna(row[\"params_dropout\"]):\n",
    "            dp_val = float(row[\"params_dropout\"])\n",
    "            dropout_flag = f\"--dropout {dp_val}\"\n",
    "    \n",
    "    # 5) Build the prediction command\n",
    "    command = (\n",
    "        f\"python predict.py \"\n",
    "        f\"--output {output_dir} \"\n",
    "        f\"--chptfolder {chptfolder} \"\n",
    "        f\"--normalize \"\n",
    "        f\"--splits {split_index} {split_index} \"  # for a single split\n",
    "        f\"{arch_flag} \"\n",
    "        f\"{hidden_dim_flag} \"\n",
    "        f\"{dropout_flag}\"\n",
    "    )\n",
    "    \n",
    "    print(\"Running command:\", command)\n",
    "    os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
