{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978d287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns: ['scan_id', 'target', 'external_prob', 'scratch_prob', 'transfer_prob']\n",
      "AUC External: 0.9056140350877192\n",
      "AUC Scratch: 0.8760818713450292\n",
      "AUC Transfer: 0.9397076023391813\n",
      "\n",
      "DeLong test External vs. Scratch:\n",
      "AUC External: 0.104, AUC Scratch: 0.104, p-value: 1.0000\n",
      "\n",
      "DeLong test External vs. Transfer:\n",
      "AUC External: 0.104, AUC Transfer: 0.104, p-value: 1.0000\n",
      "\n",
      "DeLong test Scratch vs. Transfer:\n",
      "AUC Scratch: 0.104, AUC Transfer: 0.104, p-value: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- DeLong test functions ---\n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"\n",
    "    Computes midranks.\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    T = np.empty(len(x), dtype=float)\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        j = i\n",
    "        while j < len(x) and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        # assign the average rank for tied values\n",
    "        T[i:j] = 0.5 * (i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(len(x), dtype=float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    Fast DeLong method for ROC AUC variance estimation.\n",
    "    predictions_sorted_transposed: 2D array (k x m) where each row corresponds to a classifierâ€™s predictions (here k=1).\n",
    "    label_1_count: number of positive samples.\n",
    "    \"\"\"\n",
    "    m = predictions_sorted_transposed.shape[1]\n",
    "    n = label_1_count\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "    tx = np.empty((k, n))\n",
    "    ty = np.empty((k, m - n))\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(predictions_sorted_transposed[r, :n])\n",
    "        ty[r, :] = compute_midrank(predictions_sorted_transposed[r, n:])\n",
    "    tz = np.hstack((tx, ty))\n",
    "    aucs = tx.sum(axis=1) / (n * (m - n))\n",
    "    # variance estimation (per DeLong)\n",
    "    v01 = (tz[:, :n] - np.expand_dims(aucs, axis=1)) ** 2\n",
    "    sx = np.sum(v01, axis=1) / (n - 1)\n",
    "    return aucs, sx\n",
    "\n",
    "def delong_roc_variance(y_true, predictions):\n",
    "    # Separate positive and negative predictions\n",
    "    pos_preds = predictions[y_true == 1]\n",
    "    neg_preds = predictions[y_true == 0]\n",
    "    m = len(pos_preds) + len(neg_preds)\n",
    "    n = len(pos_preds)\n",
    "    # Combine them in an array where positives come first\n",
    "    predictions_combined = np.concatenate([pos_preds, neg_preds])\n",
    "    predictions_combined = predictions_combined.reshape(1, -1)\n",
    "    aucs, sx = fastDeLong(predictions_combined, n)\n",
    "    return aucs[0], sx[0]\n",
    "\n",
    "\n",
    "def delong_roc_test(y_true, preds1, preds2):\n",
    "    \"\"\"\n",
    "    Compares two sets of prediction scores using the DeLong test.\n",
    "    Returns:\n",
    "      p_value, auc1, auc2\n",
    "    \"\"\"\n",
    "    auc1, var1 = delong_roc_variance(y_true, preds1)\n",
    "    auc2, var2 = delong_roc_variance(y_true, preds2)\n",
    "    auc_diff = auc1 - auc2\n",
    "    # For a paired test the variance of the difference ideally uses the covariance;\n",
    "    # here we assume independence for simplicity.\n",
    "    var_diff = var1 + var2  \n",
    "    z = np.abs(auc_diff) / np.sqrt(var_diff)\n",
    "    p_value = 2 * (1 - norm.cdf(z))\n",
    "    return p_value, auc1, auc2\n",
    "\n",
    "# %% [code]\n",
    "# Define file paths for each ensemble file\n",
    "external_avg_path = r\"C:\\Users\\alime\\Dropbox (AMC)\\Mon PC (DESKTOP-RG9FHVT)\\Desktop\\Deauville\\Deauville_DeepLearning\\prediction\\external\\ensemble_avg.csv\"\n",
    "scratch_avg_path  = r\"C:\\Users\\alime\\Dropbox (AMC)\\Mon PC (DESKTOP-RG9FHVT)\\Desktop\\Deauville\\Deauville_DeepLearning\\prediction\\scratch\\Run10\\ensemble_avg.csv\"\n",
    "transfer_avg_path = r\"C:\\Users\\alime\\Dropbox (AMC)\\Mon PC (DESKTOP-RG9FHVT)\\Desktop\\Deauville\\Deauville_DeepLearning\\prediction\\transfer\\Run12\\ensemble_avg.csv\"\n",
    "\n",
    "# Load each CSV\n",
    "df_external = pd.read_csv(external_avg_path)\n",
    "df_scratch  = pd.read_csv(scratch_avg_path)\n",
    "df_transfer = pd.read_csv(transfer_avg_path)\n",
    "\n",
    "# Each file has columns: scan_id, probs_model0, target, ..., ensemble_prob, pred_label.\n",
    "# We assume the \"target\" is identical across files.\n",
    "# Merge on 'scan_id'\n",
    "df_merged = df_external[['scan_id', 'target', 'ensemble_prob']].copy()\n",
    "df_merged = df_merged.merge(df_scratch[['scan_id', 'ensemble_prob']], on='scan_id', how='inner', suffixes=('_external', '_scratch'))\n",
    "df_merged = df_merged.merge(df_transfer[['scan_id', 'ensemble_prob']], on='scan_id', how='inner')\n",
    "# After the second merge, the transfer column gets a suffix automatically, e.g., 'ensemble_prob' or 'ensemble_prob_transfer'.\n",
    "if 'ensemble_prob_transfer' in df_merged.columns:\n",
    "    df_merged.rename(columns={'ensemble_prob_transfer': 'ensemble_prob_transfer'}, inplace=True)\n",
    "else:\n",
    "    df_merged.rename(columns={'ensemble_prob': 'ensemble_prob_transfer'}, inplace=True)\n",
    "\n",
    "# Now rename columns for clarity:\n",
    "df_merged.rename(columns={\n",
    "    'ensemble_prob_external': 'external_prob',\n",
    "    'ensemble_prob_scratch': 'scratch_prob',\n",
    "    'ensemble_prob_transfer': 'transfer_prob'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Merged columns:\", df_merged.columns.tolist())\n",
    "\n",
    "# %% [code]\n",
    "# Compute AUCs for each model ensemble\n",
    "y_true = df_merged['target'].values\n",
    "auc_external = roc_auc_score(y_true, df_merged['external_prob'].values)\n",
    "auc_scratch  = roc_auc_score(y_true, df_merged['scratch_prob'].values)\n",
    "auc_transfer = roc_auc_score(y_true, df_merged['transfer_prob'].values)\n",
    "\n",
    "print(\"AUC External:\", auc_external)\n",
    "print(\"AUC Scratch:\", auc_scratch)\n",
    "print(\"AUC Transfer:\", auc_transfer)\n",
    "\n",
    "# %% [code]\n",
    "# Compare External vs. Scratch using DeLong test\n",
    "p_ext_scratch, auc_ext, auc_scratch_calc = delong_roc_test(y_true, \n",
    "                                                          df_merged['external_prob'].values,\n",
    "                                                          df_merged['scratch_prob'].values)\n",
    "print(\"\\nDeLong test External vs. Scratch:\")\n",
    "print(f\"AUC External: {auc_ext:.3f}, AUC Scratch: {auc_scratch_calc:.3f}, p-value: {p_ext_scratch:.4f}\")\n",
    "\n",
    "# Compare External vs. Transfer using DeLong test\n",
    "p_ext_transfer, auc_ext, auc_transfer_calc = delong_roc_test(y_true, \n",
    "                                                            df_merged['external_prob'].values,\n",
    "                                                            df_merged['transfer_prob'].values)\n",
    "print(\"\\nDeLong test External vs. Transfer:\")\n",
    "print(f\"AUC External: {auc_ext:.3f}, AUC Transfer: {auc_transfer_calc:.3f}, p-value: {p_ext_transfer:.4f}\")\n",
    "\n",
    "# Compare Scratch vs. Transfer using DeLong test\n",
    "p_scratch_transfer, auc_scratch_calc, auc_transfer_calc = delong_roc_test(y_true, \n",
    "                                                                        df_merged['scratch_prob'].values,\n",
    "                                                                        df_merged['transfer_prob'].values)\n",
    "print(\"\\nDeLong test Scratch vs. Transfer:\")\n",
    "print(f\"AUC Scratch: {auc_scratch_calc:.3f}, AUC Transfer: {auc_transfer_calc:.3f}, p-value: {p_scratch_transfer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c466bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC External: 0.9056140350877192\n",
      "AUC Scratch: 0.8760818713450292\n",
      "AUC Transfer: 0.9397076023391813\n",
      "\n",
      "DeLong test External vs. Scratch:\n",
      "AUC External: 0.906, AUC Scratch: 0.876, p-value: 0.4596\n",
      "\n",
      "DeLong test External vs. Transfer:\n",
      "AUC External: 0.906, AUC Transfer: 0.940, p-value: 0.3200\n",
      "\n",
      "DeLong test Scratch vs. Transfer:\n",
      "AUC Scratch: 0.876, AUC Transfer: 0.940, p-value: 0.0845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "\n",
    "def delong_auc_variance(y_true, y_scores):\n",
    "    # Separate positive and negative scores\n",
    "    pos_scores = y_scores[y_true == 1]\n",
    "    neg_scores = y_scores[y_true == 0]\n",
    "    n1 = len(pos_scores)\n",
    "    n2 = len(neg_scores)\n",
    "    \n",
    "    # Combine scores and compute average ranks\n",
    "    all_scores = np.concatenate([pos_scores, neg_scores])\n",
    "    ranks = pd.Series(all_scores).rank(method='average').values\n",
    "    # Ranks for positive scores\n",
    "    ranks_pos = ranks[:n1]\n",
    "    \n",
    "    auc = (np.sum(ranks_pos) - n1*(n1+1)/2) / (n1*n2)\n",
    "    Q1 = auc / (2 - auc)\n",
    "    Q2 = 2 * auc**2 / (1 + auc)\n",
    "    var_auc = (auc * (1 - auc) + (n1 - 1)*(Q1 - auc**2) + (n2 - 1)*(Q2 - auc**2)) / (n1*n2)\n",
    "    return auc, var_auc\n",
    "\n",
    "def delong_roc_test(y_true, preds1, preds2):\n",
    "    \"\"\"\n",
    "    Compare two sets of prediction scores using DeLong test.\n",
    "    Returns p-value, auc1, auc2.\n",
    "    \"\"\"\n",
    "    auc1, var1 = delong_auc_variance(y_true, preds1)\n",
    "    auc2, var2 = delong_auc_variance(y_true, preds2)\n",
    "    auc_diff = auc1 - auc2\n",
    "    var_diff = var1 + var2  # Note: this assumes independence (simplification)\n",
    "    z = np.abs(auc_diff) / np.sqrt(var_diff)\n",
    "    p_value = 2 * (1 - norm.cdf(z))\n",
    "    return p_value, auc1, auc2\n",
    "\n",
    "# Compute AUCs via roc_auc_score (these are our reference values)\n",
    "y_true = df_merged['target'].values\n",
    "auc_external = roc_auc_score(y_true, df_merged['external_prob'].values)\n",
    "auc_scratch  = roc_auc_score(y_true, df_merged['scratch_prob'].values)\n",
    "auc_transfer = roc_auc_score(y_true, df_merged['transfer_prob'].values)\n",
    "\n",
    "print(\"AUC External:\", auc_external)\n",
    "print(\"AUC Scratch:\", auc_scratch)\n",
    "print(\"AUC Transfer:\", auc_transfer)\n",
    "\n",
    "# Compare models using the corrected DeLong test:\n",
    "p_ext_scratch, auc_ext, auc_scratch_calc = delong_roc_test(y_true, \n",
    "                                                          df_merged['external_prob'].values,\n",
    "                                                          df_merged['scratch_prob'].values)\n",
    "print(\"\\nDeLong test External vs. Scratch:\")\n",
    "print(f\"AUC External: {auc_ext:.3f}, AUC Scratch: {auc_scratch_calc:.3f}, p-value: {p_ext_scratch:.4f}\")\n",
    "\n",
    "p_ext_transfer, auc_ext, auc_transfer_calc = delong_roc_test(y_true, \n",
    "                                                            df_merged['external_prob'].values,\n",
    "                                                            df_merged['transfer_prob'].values)\n",
    "print(\"\\nDeLong test External vs. Transfer:\")\n",
    "print(f\"AUC External: {auc_ext:.3f}, AUC Transfer: {auc_transfer_calc:.3f}, p-value: {p_ext_transfer:.4f}\")\n",
    "\n",
    "p_scratch_transfer, auc_scratch_calc, auc_transfer_calc = delong_roc_test(y_true, \n",
    "                                                                        df_merged['scratch_prob'].values,\n",
    "                                                                        df_merged['transfer_prob'].values)\n",
    "print(\"\\nDeLong test Scratch vs. Transfer:\")\n",
    "print(f\"AUC Scratch: {auc_scratch_calc:.3f}, AUC Transfer: {auc_transfer_calc:.3f}, p-value: {p_scratch_transfer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5dcb2",
   "metadata": {},
   "source": [
    "In summary, based on the DeLong test:\n",
    "\n",
    "None of the differences are statistically significant.\n",
    "\n",
    "Even the largest difference (between Scratch and Transfer) has a p-value of 0.0845, which suggests that while there may be a trend (with the transfer model performing better numerically), the evidence isnâ€™t strong enough (at the 5% level) to confidently state that one modelâ€™s AUC is truly higher than the otherâ€™s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
